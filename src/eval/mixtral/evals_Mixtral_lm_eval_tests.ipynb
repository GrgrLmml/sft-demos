{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zfjv9nzQSyZ0",
        "outputId": "9c0048b7-2e85-48d8-e80c-c8aeb361aab1"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers peft torch accelerate einops sentencepiece bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Bw1lhjhYhw1"
      },
      "outputs": [],
      "source": [
        "# clone repository\n",
        "#!git clone https://github.com/EleutherAI/lm-evaluation-harness.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWagqq3cwLgF"
      },
      "outputs": [],
      "source": [
        "# change to repo directory\n",
        "#import os\n",
        "#os.chdir(\"/content/lm-evaluation-harness\")\n",
        "# install\n",
        "#!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTQQFRQLrkp1"
      },
      "outputs": [],
      "source": [
        "#!pip list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_s9gEgK_SxD3",
        "outputId": "205f4126-f8c0-461f-9454-5e93d863f4f0"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWp8s9SepDPO"
      },
      "outputs": [],
      "source": [
        "# arc_challenge,hellaswag,truthfulqa_mc2,winogrande,gsm8k \\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaDYN1czqPps",
        "outputId": "92901444-8494-4bb8-b49a-6d9b3dcacabe"
      },
      "outputs": [],
      "source": [
        "!lm_eval --model hf \\\n",
        "    --model_args pretrained=mistralai/Mixtral-8x7B-v0.1,dtype=\"bfloat16\",load_in_4bit=True,peft=dfurman/Mixtral-8x7B-Instruct-v0.1 \\\n",
        "    --tasks arc_challenge \\\n",
        "    --batch_size 2 \\\n",
        "    --write_out \\\n",
        "    --output_path open_leaderboard_minus_mmlu_lm_eval.json \\\n",
        "    --device cuda:0 \\\n",
        "    --num_fewshot 5 \\\n",
        "    --verbosity DEBUG\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onoPU1Cmk5EF",
        "outputId": "d4bc8a4b-95f7-43c4-b005-325f42f6e527"
      },
      "outputs": [],
      "source": [
        "!lm-eval --tasks list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tMs9GMfFtpM"
      },
      "outputs": [],
      "source": [
        "# mmlu_abstract_algebra,mmlu_anatomy,mmlu_astronomy,mmlu_business_ethics,mmlu_clinical_knowledge,mmlu_college_biology,mmlu_college_chemistry,mmlu_college_computer_science,mmlu_college_mathematics,mmlu_college_medicine,mmlu_college_physics,mmlu_computer_security,mmlu_conceptual_physics,mmlu_econometrics,mmlu_electrical_engineering,mmlu_elementary_mathematics,mmlu_formal_logic,mmlu_global_facts,mmlu_high_school_biology,mmlu_high_school_chemistry,mmlu_high_school_computer_science,mmlu_high_school_european_history,mmlu_high_school_geography,mmlu_high_school_government_and_politics,mmlu_high_school_macroeconomics,mmlu_high_school_mathematics,mmlu_high_school_microeconomics,mmlu_high_school_physics,mmlu_high_school_psychology,mmlu_high_school_statistics,mmlu_high_school_us_history,mmlu_high_school_world_history,mmlu_human_aging,mmlu_human_sexuality,mmlu_humanities,mmlu_international_law,mmlu_jurisprudence,mmlu_logical_fallacies,mmlu_machine_learning,mmlu_management,mmlu_marketing,mmlu_medical_genetics,mmlu_miscellaneous,mmlu_moral_disputes,mmlu_moral_scenarios,mmlu_nutrition,mmlu_other,mmlu_philosophy,mmlu_prehistory,mmlu_professional_accounting,mmlu_professional_law,mmlu_professional_medicine,mmlu_professional_psychology,mmlu_public_relations,mmlu_security_studies,mmlu_social_sciences,mmlu_sociology,mmlu_stem,mmlu_us_foreign_policy,mmlu_virology,mmlu_world_religions \\"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
