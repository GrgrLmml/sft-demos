{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PfoqKezQ-BQL",
    "outputId": "fedc6930-8d24-41e6-b315-9a64d0ddd992"
   },
   "outputs": [],
   "source": [
    "# To add a new cell, type '# %%'\n",
    "# To add a new markdown cell, type '# %% [markdown]'\n",
    "# %%\n",
    "import os\n",
    "\n",
    "#  [markdown]\n",
    "# ## Finetune Mistral-7b on an A100\n",
    "#\n",
    "# Welcome to this Colab notebook that shows how to fine-tune the recent Llama-2-7b model on a single GPU.\n",
    "#\n",
    "# We will leverage PEFT library from Hugging Face ecosystem, as well as QLoRA for more memory efficient finetuning\n",
    "\n",
    "# %%\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6P5_sRXq-hxO",
    "outputId": "a517c3c7-120b-418f-93eb-917b7f4bf610"
   },
   "outputs": [],
   "source": [
    "# %%\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5Wb8SN2-m9y"
   },
   "outputs": [],
   "source": [
    "#  [markdown]\n",
    "# ## Setup\n",
    "#\n",
    "# Run the cells below to setup and install the required libraries. For our experiment we will need `accelerate`, `peft`, `transformers`, `datasets` and TRL to leverage the recent [`SFTTrainer`](https://huggingface.co/docs/trl/main/en/sft_trainer). We will use `bitsandbytes` to [quantize the base model into 4bit](https://huggingface.co/blog/4bit-transformers-bitsandbytes).\n",
    "\n",
    "#\n",
    "!pip install -U bitsandbytes einops\n",
    "!pip install -U git+https://github.com/huggingface/peft\n",
    "!pip install -U git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zY6tKH4I_emz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rmeb6HfYzz8s"
   },
   "outputs": [],
   "source": [
    "peft_model_id = \"dfurman/mistral-7b-instruct-peft\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mvZDvS99kLMg"
   },
   "outputs": [],
   "source": [
    "format_template = \"You are a helpful assistant. Write a response that appropriately completes the request. {query}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CA5TwEUfz_r1",
    "outputId": "bfd7e542-e665-424b-b86a-2db9d03a1a71"
   },
   "outputs": [],
   "source": [
    "# First, format the prompt\n",
    "query = \"What is a good recipe for vegan banana bread?\"\n",
    "prompt = format_template.format(query=query)\n",
    "\n",
    "# Inference can be done using model.generate\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "with torch.autocast(\"cuda\", dtype=torch.float16):\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        # temperature=0.3,\n",
    "        return_dict_in_generate=True,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        repetition_penalty=1.2,\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(output[\"sequences\"][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9eclraDGH0UW",
    "outputId": "a4a3617a-f1bb-4d81-acc4-290c6a8eab01"
   },
   "outputs": [],
   "source": [
    "# First, format the prompt\n",
    "query = \"Write me a numbered list of things to do in New York City.\"\n",
    "prompt = format_template.format(query=query)\n",
    "\n",
    "# Inference can be done using model.generate\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "with torch.autocast(\"cuda\", dtype=torch.float16):\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        # temperature=0.3,\n",
    "        return_dict_in_generate=True,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        repetition_penalty=1.2,\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(output[\"sequences\"][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BaNWTlUTH3iT",
    "outputId": "39028250-a643-415a-f4bc-d4aa56b88ffc"
   },
   "outputs": [],
   "source": [
    "# First, format the prompt\n",
    "query = \"Write a short email inviting my friends to a dinner party on Friday. Respond succinctly.\"\n",
    "prompt = format_template.format(query=query)\n",
    "\n",
    "# Inference can be done using model.generate\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "with torch.autocast(\"cuda\", dtype=torch.float16):\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=False,\n",
    "        # temperature=0.3,\n",
    "        return_dict_in_generate=True,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        repetition_penalty=1.2,\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(output[\"sequences\"][0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nj-oywYiz1ER"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "interpreter": {
   "hash": "53aa1d185f0c4d464253b7bca5e55e34e60de52cf1459f322cf3aa8af1e32b33"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
