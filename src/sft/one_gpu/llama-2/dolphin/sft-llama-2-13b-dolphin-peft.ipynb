{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2EgqEPDQ8v6"
   },
   "source": [
    "## Finetune Llama-2-13b on an A6000\n",
    "\n",
    "Welcome to this LambdaLabs notebook that shows how to fine-tune the recent Llama-2-13b model on a single GPU.\n",
    "\n",
    "We will leverage PEFT library from Hugging Face ecosystem, as well as QLoRA for more memory efficient finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QzsG8sbwbpAS",
    "outputId": "ba15f00d-2d42-4a00-a62f-7203728d574a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2022 NVIDIA Corporation\n",
      "Built on Tue_Mar__8_18:18:20_PST_2022\n",
      "Cuda compilation tools, release 11.6, V11.6.124\n",
      "Build cuda_11.6.r11.6/compiler.31057947_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-G4l96TbnkK",
    "outputId": "877f84fd-eed2-45e9-e6a9-25bc512fa5e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jul 23 07:00:35 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    On   | 00000000:05:00.0 Off |                  Off |\n",
      "| 30%   31C    P8     7W / 300W |      1MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-tTvEF1RT3y"
   },
   "source": [
    "## Setup\n",
    "\n",
    "Run the cells below to setup and install the required libraries. For our experiment we will need `accelerate`, `peft`, `transformers`, `datasets` and TRL to leverage the recent [`SFTTrainer`](https://huggingface.co/docs/trl/main/en/sft_trainer). We will use `bitsandbytes` to [quantize the base model into 4bit](https://huggingface.co/blog/4bit-transformers-bitsandbytes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mNnkgBq7Q3EU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U trl transformers accelerate protobuf==3.19.0\n",
    "!pip install -q datasets bitsandbytes einops wandb\n",
    "!pip install -q git+https://github.com/huggingface/peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "LoHfeu-ci_b-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VS2vk8TPbrvE",
    "outputId": "083da69a-615d-4c7d-eede-7d9925e00aae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                  Version\n",
      "------------------------ --------------------\n",
      "absl-py                  0.15.0\n",
      "accelerate               0.21.0\n",
      "aiohttp                  3.8.5\n",
      "aiosignal                1.3.1\n",
      "anyio                    3.6.2\n",
      "appdirs                  1.4.3\n",
      "argon2-cffi              21.3.0\n",
      "argon2-cffi-bindings     21.2.0\n",
      "astunparse               1.6.2\n",
      "async-timeout            4.0.2\n",
      "atomicwrites             1.1.5\n",
      "attrs                    19.3.0\n",
      "Automat                  0.8.0\n",
      "Babel                    2.10.3\n",
      "backcall                 0.1.0\n",
      "beautifulsoup4           4.8.2\n",
      "bitsandbytes             0.41.0\n",
      "bleach                   3.1.1\n",
      "blinker                  1.4\n",
      "blosc                    1.7.0\n",
      "bottle                   0.12.15\n",
      "cachetools               4.0.0\n",
      "caffe                    1.0.0\n",
      "certifi                  2019.11.28\n",
      "cffi                     1.14.0\n",
      "chardet                  3.0.4\n",
      "charset-normalizer       2.1.1\n",
      "click                    8.1.6\n",
      "cloud-init               22.3.4\n",
      "cloudpickle              1.3.0\n",
      "cmake                    3.27.0\n",
      "colorama                 0.4.3\n",
      "command-not-found        0.3\n",
      "configobj                5.0.6\n",
      "constantly               15.1.0\n",
      "cryptography             2.8\n",
      "ctop                     1.0.0\n",
      "cycler                   0.10.0\n",
      "Cython                   0.29.14\n",
      "dask                     2.8.1+dfsg\n",
      "datasets                 2.13.1\n",
      "dbus-python              1.2.16\n",
      "decorator                4.4.2\n",
      "defusedxml               0.6.0\n",
      "dill                     0.3.6\n",
      "distlib                  0.3.0\n",
      "distro                   1.4.0\n",
      "distro-info              0.23ubuntu1\n",
      "docker                   4.1.0\n",
      "docker-pycreds           0.4.0\n",
      "einops                   0.6.1\n",
      "entrypoints              0.3\n",
      "et-xmlfile               1.0.1\n",
      "fastjsonschema           2.16.2\n",
      "filelock                 3.0.12\n",
      "flake8                   3.7.9\n",
      "flatbuffers              1.12\n",
      "fonttools                4.38.0\n",
      "frozenlist               1.4.0\n",
      "fsspec                   2023.6.0\n",
      "future                   0.18.2\n",
      "gast                     0.4.0\n",
      "gitdb                    4.0.10\n",
      "GitPython                3.1.32\n",
      "Glances                  3.1.3\n",
      "google-auth              1.5.1\n",
      "google-auth-oauthlib     0.4.1\n",
      "google-pasta             0.2.0\n",
      "grpcio                   1.29.1\n",
      "h5py                     2.10.0\n",
      "html5lib                 1.0.1\n",
      "htmlmin                  0.1.12\n",
      "httplib2                 0.14.0\n",
      "huggingface-hub          0.16.4\n",
      "hyperlink                19.0.0\n",
      "icdiff                   1.9.5\n",
      "idna                     2.8\n",
      "ImageHash                4.3.1\n",
      "imageio                  2.4.1\n",
      "importlib-metadata       5.0.0\n",
      "incremental              16.10.1\n",
      "influxdb                 5.2.0\n",
      "iotop                    0.6\n",
      "ipykernel                5.2.0\n",
      "ipython                  7.13.0\n",
      "ipython_genutils         0.2.0\n",
      "ipywidgets               8.0.2\n",
      "jdcal                    1.0\n",
      "jedi                     0.15.2\n",
      "Jinja2                   3.1.2\n",
      "joblib                   1.2.0\n",
      "json5                    0.9.10\n",
      "jsonpatch                1.22\n",
      "jsonpointer              2.0\n",
      "jsonschema               3.2.0\n",
      "jupyter_client           7.4.4\n",
      "jupyter-console          6.0.0\n",
      "jupyter_core             4.11.2\n",
      "jupyter-server           1.21.0\n",
      "jupyterlab               3.5.0\n",
      "jupyterlab-pygments      0.2.2\n",
      "jupyterlab_server        2.16.1\n",
      "jupyterlab-widgets       3.0.3\n",
      "kaptan                   0.5.10\n",
      "keras                    2.9.0\n",
      "Keras-Preprocessing      1.1.2\n",
      "keyring                  18.0.1\n",
      "kiwisolver               1.0.1\n",
      "language-selector        0.1\n",
      "launchpadlib             1.10.13\n",
      "lazr.restfulclient       0.14.2\n",
      "lazr.uri                 1.0.3\n",
      "libtmux                  0.8.2\n",
      "lit                      16.0.6\n",
      "locket                   0.2.0\n",
      "lxml                     4.5.0\n",
      "Mako                     1.1.0\n",
      "Markdown                 3.1.1\n",
      "MarkupSafe               2.1.1\n",
      "matplotlib               3.5.3\n",
      "mccabe                   0.6.1\n",
      "missingno                0.5.1\n",
      "mistune                  2.0.4\n",
      "more-itertools           4.2.0\n",
      "mpi4py                   3.0.3\n",
      "mpmath                   1.3.0\n",
      "multidict                6.0.4\n",
      "multimethod              1.9\n",
      "multiprocess             0.70.14\n",
      "nbclassic                0.4.7\n",
      "nbclient                 0.7.0\n",
      "nbconvert                7.2.2\n",
      "nbformat                 5.7.0\n",
      "nest-asyncio             1.5.6\n",
      "netifaces                0.10.4\n",
      "networkx                 2.4\n",
      "nose                     1.3.7\n",
      "notebook                 6.0.3\n",
      "notebook_shim            0.2.0\n",
      "numexpr                  2.7.1\n",
      "numpy                    1.23.4\n",
      "nvidia-cublas-cu11       11.10.3.66\n",
      "nvidia-cuda-cupti-cu11   11.7.101\n",
      "nvidia-cuda-nvrtc-cu11   11.7.99\n",
      "nvidia-cuda-runtime-cu11 11.7.99\n",
      "nvidia-cudnn-cu11        8.5.0.96\n",
      "nvidia-cufft-cu11        10.9.0.58\n",
      "nvidia-curand-cu11       10.2.10.91\n",
      "nvidia-cusolver-cu11     11.4.0.1\n",
      "nvidia-cusparse-cu11     11.7.4.91\n",
      "nvidia-ml-py             7.352.0\n",
      "nvidia-nccl-cu11         2.14.3\n",
      "nvidia-nvtx-cu11         11.7.91\n",
      "oauthlib                 3.1.0\n",
      "olefile                  0.46\n",
      "openpyxl                 3.0.3\n",
      "opt-einsum               3.3.0\n",
      "packaging                21.3\n",
      "pandas                   1.5.1\n",
      "pandas-profiling         3.4.0\n",
      "pandocfilters            1.4.2\n",
      "parameterized            0.7.0\n",
      "parso                    0.5.2\n",
      "partd                    1.0.0\n",
      "pathtools                0.1.2\n",
      "patsy                    0.5.3\n",
      "peft                     0.5.0.dev0\n",
      "pexpect                  4.6.0\n",
      "phik                     0.12.2\n",
      "pickleshare              0.7.5\n",
      "Pillow                   7.0.0\n",
      "pip                      22.3\n",
      "pluggy                   0.13.0\n",
      "ply                      3.11\n",
      "prometheus-client        0.7.1\n",
      "prompt-toolkit           2.0.10\n",
      "protobuf                 3.19.0\n",
      "psutil                   5.5.1\n",
      "ptyprocess               0.7.0\n",
      "py                       1.8.1\n",
      "pyarrow                  12.0.1\n",
      "pyasn1                   0.4.2\n",
      "pyasn1-modules           0.2.1\n",
      "pycodestyle              2.5.0\n",
      "pycparser                2.19\n",
      "pycryptodomex            3.6.1\n",
      "pycuda                   2019.1.2\n",
      "pydantic                 1.10.2\n",
      "pydot                    1.4.1\n",
      "pyflakes                 2.1.1\n",
      "Pygments                 2.13.0\n",
      "PyGObject                3.36.0\n",
      "pygpu                    0.7.6\n",
      "PyHamcrest               1.9.0\n",
      "pyinotify                0.9.6\n",
      "PyJWT                    1.7.1\n",
      "pymacaroons              0.13.0\n",
      "PyNaCl                   1.3.0\n",
      "pyOpenSSL                19.0.0\n",
      "pyparsing                2.4.6\n",
      "pyrsistent               0.15.5\n",
      "pyserial                 3.4\n",
      "pysmi                    0.3.2\n",
      "pysnmp                   4.4.6\n",
      "pystache                 0.5.4\n",
      "pytest                   4.6.9\n",
      "python-apt               2.0.0+ubuntu0.20.4.8\n",
      "python-dateutil          2.8.2\n",
      "python-debian            0.1.36ubuntu1\n",
      "pytools                  2019.1.1\n",
      "pytz                     2022.5\n",
      "PyWavelets               0.5.1\n",
      "PyYAML                   5.3.1\n",
      "pyzmq                    24.0.1\n",
      "regex                    2023.6.3\n",
      "requests                 2.28.1\n",
      "requests-oauthlib        1.0.0\n",
      "requests-unixsocket      0.2.0\n",
      "rsa                      4.0\n",
      "safetensors              0.3.1\n",
      "scikit-cuda              0.5.3\n",
      "scikit-image             0.16.2\n",
      "scikit-learn             0.22.2.post1\n",
      "scipy                    1.9.3\n",
      "seaborn                  0.12.1\n",
      "SecretStorage            2.3.1\n",
      "Send2Trash               1.8.0\n",
      "sentry-sdk               1.28.1\n",
      "service-identity         18.1.0\n",
      "setproctitle             1.3.2\n",
      "setuptools               45.2.0\n",
      "simplejson               3.16.0\n",
      "six                      1.14.0\n",
      "smmap                    5.0.0\n",
      "sniffio                  1.3.0\n",
      "sos                      4.4\n",
      "soupsieve                1.9.5\n",
      "ssh-import-id            5.10\n",
      "statsmodels              0.13.2\n",
      "sympy                    1.12\n",
      "systemd-python           234\n",
      "tables                   3.6.1\n",
      "tangled-up-in-unicode    0.2.0\n",
      "tensorboard              2.9.1\n",
      "tensorflow-estimator     2.9.0\n",
      "tensorflow-gpu           2.9.1\n",
      "termcolor                1.1.0\n",
      "terminado                0.17.0\n",
      "testpath                 0.4.4\n",
      "Theano                   1.0.4\n",
      "tinycss2                 1.2.1\n",
      "tmuxp                    1.5.4\n",
      "tokenizers               0.13.3\n",
      "tomli                    2.0.1\n",
      "toolz                    0.9.0\n",
      "torch                    2.0.1\n",
      "torchvision              0.13.1\n",
      "tornado                  6.2\n",
      "tqdm                     4.64.1\n",
      "traitlets                5.5.0\n",
      "transformers             4.31.0\n",
      "triton                   2.0.0\n",
      "trl                      0.4.7\n",
      "Twisted                  18.9.0\n",
      "typing_extensions        4.4.0\n",
      "ubuntu-advantage-tools   27.11.2\n",
      "ufw                      0.36\n",
      "unattended-upgrades      0.1\n",
      "urllib3                  1.26.16\n",
      "virtualenv               20.0.17\n",
      "visions                  0.7.5\n",
      "wadllib                  1.3.3\n",
      "wandb                    0.15.5\n",
      "wcwidth                  0.1.8\n",
      "webencodings             0.5.1\n",
      "websocket-client         0.53.0\n",
      "Werkzeug                 0.16.1\n",
      "wheel                    0.34.2\n",
      "widgetsnbextension       4.0.3\n",
      "wrapt                    1.11.2\n",
      "xlrd                     1.1.0\n",
      "xlwt                     1.3.0\n",
      "xxhash                   3.2.0\n",
      "yarl                     1.9.2\n",
      "zipp                     1.0.0\n",
      "zope.interface           4.7.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rnqmq7amRrU8"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "For our experiment, we will use the `ehartford/dolphin` dataset to train general purpose instruct model.\n",
    "\n",
    "\n",
    "The dataset can be found [here](https://huggingface.co/datasets/ehartford/dolphin)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0X3kHnskSWU4",
    "outputId": "440a3020-ba91-4ca4-f0f1-9a1ff0008935"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading ehartford/dolphin dataset...\n",
      "Print an example in the train dataset:\n",
      "Dataset({\n",
      "    features: ['prompt', 'response'],\n",
      "    num_rows: 100000\n",
      "})\n",
      "{'prompt': 'You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old. Continue the following story.\\n\\nEmily held and rocked Kayla as they sobbed because \\nSelect from the following.\\n +Emily really needed a good friend.\\n +Kayla really needed a good friend.', 'response': 'Once upon a time, Emily held and rocked Kayla as they both sobbed because Kayla really needed a good friend. You see, little Kayla was feeling very lonely and sad because she didn\\'t have anyone to play with or talk to.\\n\\nEmily wanted to help Kayla and be her friend, so she did something truly special. Emily decided to throw a magical friendship party for Kayla. Emily knew that this would cheer her up and help her make some new friends!\\n\\nEmily worked really hard, preparing for the big day. She bought balloons, baked cookies, and even created a treasure hunt for everyone to enjoy. She invited lots of other kids from their neighborhood, so Kayla could meet them all and find a friend to play with.\\n\\nOn the day of the party, the sun was shining, and the sky was filled with fluffy clouds. All the kids in the neighborhood were excited to come to the party and play with Kayla.\\n\\nEmily and Kayla welcomed their guests with huge smiles and warm hugs. The kids played games, laughed, and explored the treasure hunt that Emily had set up all around the backyard.\\n\\nSoon, Kayla was surrounded by new friends, chatting and laughing together. She began to feel so happy and loved. As the sun began to set and the party ended, Kayla felt like she finally had found the friends she needed.\\n\\nThat night, as Emily tucked Kayla into bed, Kayla thanked her for being such an amazing friend and throwing the best party ever. Emily smiled and said, \"That\\'s what friends are for, to help each other when we\\'re feeling sad and lonely.\"\\n\\nAnd from that day on, Emily and Kayla were the best of friends and shared many more magical adventures together. Now, Kayla knew that she always had a good friend by her side.'}\n",
      "Final train dataset:\n",
      "Dataset({\n",
      "    features: ['prompt', 'response'],\n",
      "    num_rows: 100000\n",
      "})\n",
      "{'prompt': 'You should describe the task and explain your answer. While answering a multiple choice question, first output the correct answer(s). Then explain why other answers are wrong. Think like you are answering to a five year old. This question has options. How would someone go about accomplishing this goal?\\nWhat do I do if filling gets stuck on the side of the blender when making pavlova cookies?\\n\\nOptions: (A). If filling gets stuck on the side of the blender, you can scrape it with a cookie scoop. (B). If filling gets stuck on the side of the blender, you can scrape it with a spatula.', 'response': \"The correct answer is (B). If filling gets stuck on the side of the blender, you can scrape it with a spatula.\\n\\nA spatula is the right tool to use when you need to scrape the sides of a blender because it has a flat, flexible edge that can reach all the areas easily and mix everything evenly. A cookie scoop, on the other hand, is used to get equal-sized portions of cookie dough, and it wouldn't be good for scraping blender sides.\"}\n",
      "{'prompt': 'You are an AI assistant. You will be given a task. You must generate a detailed and long answer. Добро пожаловать на ферму Пазо Фино! Мы выращиваем и тренируем лошадей Пазо Фино уже более 20 лет.\\n\\nWhich language is this?', 'response': 'This text is in Russian. It welcomes the reader to the Paso Fino farm and mentions that they have been breeding and training Paso Fino horses for over 20 years. The translation of the text in English would be: \"Welcome to the Paso Fino farm! We have been growing and training Paso Fino horses for more than 20 years.\" Paso Fino is a horse breed that is known for its smooth gait and high endurance, making them suitable for various equestrian activities.'}\n",
      "Print an example in the eval dataset:\n",
      "Dataset({\n",
      "    features: ['prompt', 'response'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "{'prompt': \"You are an AI assistant. Provide a detailed answer so user don’t need to search outside to understand the answer. Q: Is this product review positive? Title: Not worth it. Review: Someday I'll learn not to buy cheap pruners. The handle came loose on the first use. They're fine for small branches and twigs but certainly weren't made for the kind of work I had in mind. I once had a pair of pruners that lasted an entire season of cutting through thick branches--if only I could remember what brand they were! Feel free to email me if you can recommend some that will last. Answer:\\nThe answer is:\", 'response': 'No, this product review is not positive. The reviewer expresses dissatisfaction with the cheap pruners, mentioning that the handle came loose on their first use, and they were not suitable for heavy-duty work. The reviewer also highlights their past experience with better pruners and appears to be seeking a recommendation for a more durable option.'}\n",
      "Final eval dataset:\n",
      "Dataset({\n",
      "    features: ['prompt', 'response'],\n",
      "    num_rows: 10000\n",
      "})\n",
      "{'prompt': \"You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old. Write a question about the following article: Hoping to rebound from their dismal road loss to the Eagles, the Rams played their Week 2 home opener against the defending Super Bowl champions, the New York Giants. In the first quarter, St. Louis trailed early as Giants QB Eli Manning completed a 33-yard TD pass to WR Plaxico Burress. In the second quarter, the Rams responded with kicker Josh Brown getting a 54-yard field goal, yet New York answered with kicker John Carney kicking a 39-yard field goal. St. Louis would reply with Brown's 54-yard field goal, yet the Giants answered with Carney nailing a 33-yard field goal. In the third quarter, New York increased its lead with Manning completing a 10-yard TD pass to WR Amani Toomer. In the fourth quarter, the Rams tried to rally as WR Torry Holt made a 45-yard juggling, circus catch in the end zone from QB Marc Bulger, making the Rams down 20-13 with 10 minutes 46 seconds left to play. However, the Giants pulled away with Manning's 18-yard TD pass to RB Ahmad Bradshaw, DE Justin Tuck's 41-yard interception return for a touchdown, and Bradshaw's 31-yard TD run. With the loss, St. Louis fell to 0-2. Over the course of two games the defense gave up almost 1,000 yards. The Rams offense has yet to be in the red zone all season.\\n\\nQuestion about the article:\", 'response': 'What happened in the second quarter of the game between the Rams and the New York Giants after Josh Brown scored a 54-yard field goal?'}\n",
      "{'prompt': 'You are an AI assistant. You will be given a task. You must generate a detailed and long answer. Given the question: Suggest a movie title for the following movie plot: Ballad of a Soldier is in Russian with English subtitles. The opening scenes are on the battlefield in WWII perhaps Kursk. Two lone Russian forward observers are watch a squad of German tiger tanks rolling right at them one runs and is killed the other reports and starts to run. The German tank seems to be trying to run him over and save ammunition. These are real German tiger tanks everything in this movie puts you right in WWII Russia during the great patriotic war. Our hero manages to knock out two of these monster tanks and is rewarded with a 6 day leave. This is the story of that journey across Russian and the encounters with the Russian people as the all shoulder the bitter burden of this Great War. He meets a beautiful young Russian teenager they fall in love and both journey together. The landscapes, the people are a great documentary of how the Russian people fought this war and the great sacrifices they all made. The interactions and characters are all masterfully written and acted. The photography, the direction is the best of the best. The Black and White film format adds to the realism and the WWII feel. The sound track is perfect to complete this excellent film. This is a must see movie for any serious student of film making or WWII history. It is a tribute to the greatness of the Russian people and their greatest generation that paid in a river of blood, sweat and tears to defeat the Nazis during WWII.\\nThe answer is:', 'response': '\"Echoes of Bravery: A Journey Through Wartime Russia\"'}\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"ehartford/dolphin\"\n",
    "print(f\"\\nLoading {dataset_name} dataset...\")\n",
    "dataset_dolphin = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "\n",
    "# grab the first 110000 entries in an instruction format\n",
    "dataset_head = dataset_dolphin.take(110000)\n",
    "questions = []\n",
    "responses = []\n",
    "\n",
    "for row in dataset_head:\n",
    "    questions.append(f'{row[\"instruction\"]} {row[\"input\"]}')\n",
    "    responses.append(row[\"output\"])\n",
    "\n",
    "pandas_dataset_dolphin = pd.DataFrame([questions, responses]).T\n",
    "pandas_dataset_dolphin.columns = [\"prompt\", \"response\"]\n",
    "\n",
    "dataset_dolphin_train = Dataset.from_pandas(\n",
    "    pandas_dataset_dolphin.iloc[0:100000, :]\n",
    ")\n",
    "# remove old text cols\n",
    "dataset_dolphin_train = dataset_dolphin_train.remove_columns(\n",
    "    [\n",
    "        col\n",
    "        for col in dataset_dolphin_train.column_names\n",
    "        if col not in [\"prompt\", \"response\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Print an example in the train dataset:\")\n",
    "print(dataset_dolphin_train)\n",
    "print(dataset_dolphin_train[0])\n",
    "\n",
    "print(\"Final train dataset:\")\n",
    "train_dataset = dataset_dolphin_train.shuffle(seed=seed)\n",
    "print(train_dataset)\n",
    "print(train_dataset[0])\n",
    "print(train_dataset[-1])\n",
    "\n",
    "dataset_dolphin_eval = Dataset.from_pandas(pandas_dataset_dolphin.iloc[100000:, :])\n",
    "# remove old text cols\n",
    "dataset_dolphin_eval = dataset_dolphin_eval.remove_columns(\n",
    "    [\n",
    "        col\n",
    "        for col in dataset_dolphin_eval.column_names\n",
    "        if col not in [\"prompt\", \"response\"]\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Print an example in the eval dataset:\")\n",
    "print(dataset_dolphin_eval)\n",
    "print(dataset_dolphin_eval[0])\n",
    "\n",
    "print(\"Final eval dataset:\")\n",
    "eval_dataset = dataset_dolphin_eval.shuffle(seed=seed)\n",
    "print(eval_dataset)\n",
    "print(eval_dataset[0])\n",
    "print(eval_dataset[-1])\n",
    "\n",
    "# let's now write a function to format the dataset for instruction fine-tuning\n",
    "# we will use the mpt-instruct model docs format\n",
    "# see https://huggingface.co/docs/trl/main/en/sft_trainer#format-your-input-prompts for docs\n",
    "\n",
    "def formatting_prompts_func(dataset):\n",
    "    instructions = []\n",
    "    for i in range(len(dataset[\"prompt\"])):\n",
    "        text = f\"{dataset['prompt'][i]}\\n{dataset['response'][i]}\"\n",
    "        instructions.append(text)\n",
    "    return instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjOMoSbGSxx9"
   },
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjB0WAqFSzlD"
   },
   "source": [
    "In this section we will load the [Falcon 7B model](https://huggingface.co/tiiuae/falcon-7b), quantize it in 4bit and attach LoRA adapters on it. Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5lW71_MUya3",
    "outputId": "818fb495-7128-47a3-d8a6-9e4529c661bc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85fb3d0f185746e2880d80753a550018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "7XopduFDVNoE"
   },
   "outputs": [],
   "source": [
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105,
     "referenced_widgets": [
      "f2f2e64787e74578940f377dbdb1efa4",
      "edafb7765fe14074b36ca9704575866e",
      "c07ddaf350e64175bdf8a7b333047c0f",
      "e121d322679b4e119f3b58155f620d28",
      "638a6cd88e1747e39bbf83556813fefc",
      "b3f24d74c8d242e48849a78b8b7ead8a",
      "fac430a356434f2bbf44b8c360059c4c",
      "348c9cdf8f5841e48bee5d1fe9091929",
      "ded1fe42fc054acda58b710c4d18c618",
      "cdcc7ffb902d4e95b3260986b2b0b7d4",
      "716a1b9f20b64686ad19e3b4b4b7f82e"
     ]
    },
    "id": "ZwXZbQ2dSwzI",
    "outputId": "790efa3b-292c-45f8-f910-6be8555c1b6e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d428e68e9b2944fcb5e522415ee516ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/631 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ac630e0d3148c7bb47d6215bb9a2a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fetensors.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45916defbc5e474985ac36fc87a4fb03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b01c584c76d4dceb710e126b4f824a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00003.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5d9b79c9f544732b2161cec1df07c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00003.safetensors:   0%|          | 0.00/9.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0d75fc84044eff960d7793e4417bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00003.safetensors:   0%|          | 0.00/6.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b182960246c045239aa2549c0a4a38cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08b2fcd52a542db8268125a46582ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/197 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-13b-hf\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    use_auth_token=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7N0Mz8F9Vnrf",
    "outputId": "52fbaf00-7567-491e-f54d-b6b93d53d413"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 5120, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-39): 40 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=5120, out_features=5120, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=5120, out_features=13824, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=13824, out_features=5120, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=5120, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNqIYtQcUBSm"
   },
   "source": [
    "Let's also load the tokenizer below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XDS2yYmlUAD6",
    "outputId": "c1413d76-d3da-4b5c-8f3d-3306e7509140"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b69a1f0b42412194b92278c97cba0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/749 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31c31b6703af4baaaf102bb9024ad246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e50aa41d7e554039a41c3261e8903b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710212ab2914452eb49215568861fc53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NuAx3zBeUL1q"
   },
   "source": [
    "Below we will load the configuration file in order to create the LoRA model. According to QLoRA paper, it is important to consider all linear layers in the transformer block for maximum performance. Therefore we will add `q_proj`, `k_proj`, `v_proj`, `o_proj` layers in the target modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "dQdvjTYTT1vQ"
   },
   "outputs": [],
   "source": [
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 64\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzsYHLwIZoLm"
   },
   "source": [
    "## Loading the trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTBJVE4PaJwK"
   },
   "source": [
    "Here we will use the [`SFTTrainer` from TRL library](https://huggingface.co/docs/trl/main/en/sft_trainer) that gives a wrapper around transformers `Trainer` to easily fine-tune models on instruction based datasets using PEFT adapters. Let's first load the training arguments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "53rMFF6ojP2w"
   },
   "outputs": [],
   "source": [
    "output_dir = \"./results\"\n",
    "num_train_epochs = 1\n",
    "auto_find_batch_size = True\n",
    "gradient_accumulation_steps = 1\n",
    "optim = \"paged_adamw_32bit\"\n",
    "save_strategy = \"epoch\"\n",
    "learning_rate = 2e-4\n",
    "lr_scheduler_type = \"linear\"\n",
    "warmup_ratio = 0.03\n",
    "logging_strategy = \"steps\"\n",
    "logging_steps = 25\n",
    "do_eval = True\n",
    "evaluation_strategy = \"steps\"\n",
    "prediction_loss_only = True\n",
    "eval_steps = 0.2\n",
    "bf16 = True\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    auto_find_batch_size=auto_find_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_strategy=save_strategy,\n",
    "    learning_rate=learning_rate,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    logging_strategy=logging_strategy,\n",
    "    logging_steps=logging_steps,\n",
    "    # do_eval=do_eval,\n",
    "    # evaluation_strategy=evaluation_strategy,\n",
    "    prediction_loss_only=prediction_loss_only,\n",
    "    eval_steps=eval_steps,\n",
    "    bf16=bf16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3t6b2TkcJwy"
   },
   "source": [
    "Then finally pass everthing to the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91,
     "referenced_widgets": [
      "e1d56de091d142a3978b0b656b8093a5",
      "ecacb9aa832c4d8b9054a3be6b4d2105",
      "22fe50601d794f1c861e05c1a66bed91",
      "4167a121f81449c689eb55f848efd897",
      "2efb25d2e2d3447c816105ac3b7d11c5",
      "51f7f3991a0e458a9d938d096155cb05",
      "27495f76d796429fba766b48b3effd17",
      "0ffd2df206fa4a47b7e06f77910aa7a1",
      "10a5ef3e09e1491cb66844ed04fd9063",
      "26bf4df9a8b543b78df6a4023bfaf772",
      "2e49b20362314abda9a6ed71c4a8e504"
     ]
    },
    "id": "TNeOBgZeTl2H",
    "outputId": "a84dbd9f-1bd8-40a0-aadd-cd41d11bb6d4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_seq_length = 4096\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    # eval_dataset=eval_dataset,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWplqqDjb3sS"
   },
   "source": [
    "We will also pre-process the model by upcasting the layer norms in float 32 for more stable training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "7OyIvEx7b1GT"
   },
   "outputs": [],
   "source": [
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JApkSrCcL3O"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjvisllacNZM"
   },
   "source": [
    "Now let's train the model! Simply call `trainer.train()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "_kbS7nRxcMt7",
    "outputId": "aef9c552-502e-40de-9567-bdf6a6979852"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ubuntu/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/wandb/run-20230723_071649-pohixish</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dryanfurman/huggingface/runs/pohixish' target=\"_blank\">dulcet-blaze-13</a></strong> to <a href='https://wandb.ai/dryanfurman/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dryanfurman/huggingface' target=\"_blank\">https://wandb.ai/dryanfurman/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dryanfurman/huggingface/runs/pohixish' target=\"_blank\">https://wandb.ai/dryanfurman/huggingface/runs/pohixish</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='12500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   14/12500 02:16 < 39:34:28, 0.09 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   40/25000 03:28 < 37:58:50, 0.18 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.735200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1973' max='50000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1973/50000 1:28:09 < 35:48:03, 0.37 it/s, Epoch 0.04/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.756700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.770700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.754500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.766900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.794400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.510900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.421800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.498300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.433500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.417400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.391600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.513800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>1.507400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.498100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>1.276800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.349900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>1.385400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.372500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>1.348500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525</td>\n",
       "      <td>1.368400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.300800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575</td>\n",
       "      <td>1.326400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.306500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625</td>\n",
       "      <td>1.410700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.323100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675</td>\n",
       "      <td>1.368100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.384700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725</td>\n",
       "      <td>1.359400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.326900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775</td>\n",
       "      <td>1.304000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.333800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825</td>\n",
       "      <td>1.270900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.338000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875</td>\n",
       "      <td>1.427400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.317600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925</td>\n",
       "      <td>1.248500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.166000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975</td>\n",
       "      <td>1.399600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.372600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025</td>\n",
       "      <td>1.300700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.236500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075</td>\n",
       "      <td>1.322600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.284700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125</td>\n",
       "      <td>1.379500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.372200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175</td>\n",
       "      <td>1.288300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.222700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225</td>\n",
       "      <td>1.316400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.266600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275</td>\n",
       "      <td>1.281200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.388200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325</td>\n",
       "      <td>1.398200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.353400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375</td>\n",
       "      <td>1.284400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425</td>\n",
       "      <td>1.243200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.166900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475</td>\n",
       "      <td>1.247200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.382600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1525</td>\n",
       "      <td>1.239700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.333100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1575</td>\n",
       "      <td>1.205100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.182200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1625</td>\n",
       "      <td>1.179600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.326100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1675</td>\n",
       "      <td>1.318200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.192200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1725</td>\n",
       "      <td>1.227300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.190200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1775</td>\n",
       "      <td>1.280900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.233600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1825</td>\n",
       "      <td>1.429200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.311800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>1.314000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.225200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1925</td>\n",
       "      <td>1.329600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.241900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "# Tracking run with wandb version 0.15.5\n",
    "# Run data is saved locally in /home/ubuntu/wandb/run-20230723_071649-pohixish\n",
    "# Syncing run dulcet-blaze-13 to Weights & Biases (docs)\n",
    "# View project at https://wandb.ai/dryanfurman/huggingface\n",
    "# View run at https://wandb.ai/dryanfurman/huggingface/runs/pohixish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, the model should converge nicely as follows:\n",
    "\n",
    "![image](https://raw.githubusercontent.com/daniel-furman/sft-demos/main/assets/jul_22_23_3_15_00_log_loss_curves_llama-2-7b-guanaco.png)\n",
    "\n",
    "The `SFTTrainer` also takes care of properly saving only the adapters during training instead of saving the entire model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload model to HF repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# peft_model_id = \"results/checkpoint-4923\"\n",
    "# peft_model_id = \"dfurman/llama-2-13b-guanaco-peft\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nXxqUp62aH_U"
   },
   "outputs": [],
   "source": [
    "# push to hub\n",
    "model_id_load = \"dfurman/llama-2-13b-guanaco-peft\"\n",
    "\n",
    "# tokenizer\n",
    "tokenizer.push_to_hub(model_id_load, use_auth_token=True)\n",
    "# safetensors\n",
    "model.push_to_hub(model_id_load, use_auth_token=True, safe_serialization=True)\n",
    "# torch tensors\n",
    "model.push_to_hub(model_id_load, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text generation function\n",
    "\n",
    "\n",
    "def llama_generate(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 128,\n",
    "    temperature: int = 1.0,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Initialize the pipeline\n",
    "    Uses Hugging Face GenerationConfig defaults\n",
    "        https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "    Args:\n",
    "        model (transformers.AutoModelForCausalLM): Falcon model for text generation\n",
    "        tokenizer (transformers.AutoTokenizer): Tokenizer for model\n",
    "        prompt (str): Prompt for text generation\n",
    "        max_new_tokens (int, optional): Max new tokens after the prompt to generate. Defaults to 128.\n",
    "        temperature (float, optional): The value used to modulate the next token probabilities.\n",
    "            Defaults to 1.0\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        [prompt],\n",
    "        return_tensors=\"pt\",\n",
    "        return_token_type_ids=False,\n",
    "    ).to(\n",
    "        device\n",
    "    )  # tokenize inputs, load on device\n",
    "\n",
    "    # when running Torch modules in lower precision, it is best practice to use the torch.autocast context manager.\n",
    "    with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "        response = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            return_dict_in_generate=True,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    decoded_output = tokenizer.decode(\n",
    "        response[\"sequences\"][0],\n",
    "        skip_special_tokens=True,\n",
    "    )  # grab output in natural language\n",
    "\n",
    "    return decoded_output[len(prompt) :]  # remove prompt from output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import time\n",
    "\n",
    "prompt = \"You are a helpful assistant. Write me a long list of things to do in San Francisco:\\n\"\n",
    "\n",
    "runtimes = []\n",
    "for i in tqdm.tqdm(range(25)):\n",
    "    start = time.time()\n",
    "    response = llama_generate(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.92,\n",
    "    )\n",
    "    end = time.time()\n",
    "    runtimes.append(end - start)\n",
    "    assert len(tokenizer.encode(response)) == 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_runtime = torch.mean(torch.tensor(runtimes)).item()\n",
    "print(f\"Runtime avg in seconds: {avg_runtime}\")  # time in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ffd2df206fa4a47b7e06f77910aa7a1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "10a5ef3e09e1491cb66844ed04fd9063": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "22fe50601d794f1c861e05c1a66bed91": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0ffd2df206fa4a47b7e06f77910aa7a1",
      "max": 518,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_10a5ef3e09e1491cb66844ed04fd9063",
      "value": 518
     }
    },
    "26bf4df9a8b543b78df6a4023bfaf772": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "27495f76d796429fba766b48b3effd17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2e49b20362314abda9a6ed71c4a8e504": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2efb25d2e2d3447c816105ac3b7d11c5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "348c9cdf8f5841e48bee5d1fe9091929": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4167a121f81449c689eb55f848efd897": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_26bf4df9a8b543b78df6a4023bfaf772",
      "placeholder": "​",
      "style": "IPY_MODEL_2e49b20362314abda9a6ed71c4a8e504",
      "value": " 518/518 [00:00&lt;00:00, 2520.71 examples/s]"
     }
    },
    "51f7f3991a0e458a9d938d096155cb05": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "638a6cd88e1747e39bbf83556813fefc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "716a1b9f20b64686ad19e3b4b4b7f82e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b3f24d74c8d242e48849a78b8b7ead8a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c07ddaf350e64175bdf8a7b333047c0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_348c9cdf8f5841e48bee5d1fe9091929",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ded1fe42fc054acda58b710c4d18c618",
      "value": 2
     }
    },
    "cdcc7ffb902d4e95b3260986b2b0b7d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ded1fe42fc054acda58b710c4d18c618": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e121d322679b4e119f3b58155f620d28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cdcc7ffb902d4e95b3260986b2b0b7d4",
      "placeholder": "​",
      "style": "IPY_MODEL_716a1b9f20b64686ad19e3b4b4b7f82e",
      "value": " 2/2 [00:11&lt;00:00,  5.00s/it]"
     }
    },
    "e1d56de091d142a3978b0b656b8093a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ecacb9aa832c4d8b9054a3be6b4d2105",
       "IPY_MODEL_22fe50601d794f1c861e05c1a66bed91",
       "IPY_MODEL_4167a121f81449c689eb55f848efd897"
      ],
      "layout": "IPY_MODEL_2efb25d2e2d3447c816105ac3b7d11c5"
     }
    },
    "ecacb9aa832c4d8b9054a3be6b4d2105": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_51f7f3991a0e458a9d938d096155cb05",
      "placeholder": "​",
      "style": "IPY_MODEL_27495f76d796429fba766b48b3effd17",
      "value": "Map: 100%"
     }
    },
    "edafb7765fe14074b36ca9704575866e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3f24d74c8d242e48849a78b8b7ead8a",
      "placeholder": "​",
      "style": "IPY_MODEL_fac430a356434f2bbf44b8c360059c4c",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "f2f2e64787e74578940f377dbdb1efa4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_edafb7765fe14074b36ca9704575866e",
       "IPY_MODEL_c07ddaf350e64175bdf8a7b333047c0f",
       "IPY_MODEL_e121d322679b4e119f3b58155f620d28"
      ],
      "layout": "IPY_MODEL_638a6cd88e1747e39bbf83556813fefc"
     }
    },
    "fac430a356434f2bbf44b8c360059c4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
