{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5a21997-d4b7-4dbb-a25d-1740548a4780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add a new cell, type ''\n",
    "# To add a new markdown cell, type ' [markdown]'\n",
    "\n",
    "import os\n",
    "\n",
    "#  [markdown]\n",
    "# ## Finetuned on 4x A100 80GBs\n",
    "#\n",
    "# We will leverage PEFT library from Hugging Face ecosystem, as well as QLoRA for more memory efficient finetuning\n",
    "\n",
    "\n",
    "# os.system(\"nvcc --version\")\n",
    "# os.system(\"nvidia-smi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2faf95ad-7aa2-4579-a205-5decd48bed3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: protobuf 3.20.3\n",
      "Uninstalling protobuf-3.20.3:\n",
      "  Would remove:\n",
      "    /usr/local/lib/python3.10/dist-packages/google/protobuf/*\n",
      "    /usr/local/lib/python3.10/dist-packages/protobuf-3.20.3-py3.10-nspkg.pth\n",
      "    /usr/local/lib/python3.10/dist-packages/protobuf-3.20.3.dist-info/*\n",
      "Proceed (Y/n)?   Successfully uninstalled protobuf-3.20.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0myes: standard output: Broken pipe\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#  [markdown]\n",
    "# ## Setup\n",
    "#\n",
    "# Run the cells below to setup and install the required libraries. For our experiment we will need `accelerate`, `peft`, `transformers`, `datasets` and TRL to leverage the recent [`SFTTrainer`](https://huggingface.co/docs/trl/main/en/sft_trainer). We will use `bitsandbytes` to [quantize the base model into 4bit](https://huggingface.co/blog/4bit-transformers-bitsandbytes).\n",
    "\n",
    "#\n",
    "os.system(\"yes | pip uninstall protobuf\")\n",
    "os.system(\n",
    "    \"pip install -q -U accelerate datasets bitsandbytes einops wandb sentencepiece protobuf==3.20.* transformers\"\n",
    ")\n",
    "os.system(\"pip install -q -U git+https://github.com/huggingface/peft\")\n",
    "os.system(\"pip install -q -U git+https://github.com/huggingface/trl\")\n",
    "\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import copy\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe8e9cf-b4f3-4943-a742-97d7d81fe205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c798893f148a42f0aafc2b04886d9e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/104926 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4228 > 4096). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 104926/104926 [00:36<00:00, 2873.09it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1530dfad58ee4817bb67aecb4426aa04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a02a0909c134af5820c7194ac83eb70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04ad00522d34297b3cce0fe447e3da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/wandb/run-20240302_164255-4fqxqjef</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dryanfurman/huggingface/runs/4fqxqjef' target=\"_blank\">lucky-bird-101</a></strong> to <a href='https://wandb.ai/dryanfurman/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dryanfurman/huggingface' target=\"_blank\">https://wandb.ai/dryanfurman/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dryanfurman/huggingface/runs/4fqxqjef' target=\"_blank\">https://wandb.ai/dryanfurman/huggingface/runs/4fqxqjef</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:135: UserWarning: Could not find response key ` [/INST]` in the following instance: <|endoftext|>[INST] Generate a sentence with every word starting with a single letter of the alphabet, starting with 'b' and going in alphabetical order. [/INST]\"Brave cats daringly explored forests, gracefully hopping into jackal's kingdom, leaping mightily near ominous pits, quickly running swiftly towards unexpected vistas, while xenophobic yellow zebras zigzagged.\"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|> This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:135: UserWarning: Could not find response key ` [/INST]` in the following instance: <|endoftext|>[INST] Given an integer `n`, return **any** array containing `n` **unique** integers such that they add up to `0`.\n",
      "\n",
      "**Example 1:**\n",
      "\n",
      "**Input:** n = 5\n",
      "**Output:** \\[-7,-1,1,3,4\\]\n",
      "**Explanation:** These arrays also are accepted \\[-5,-1,1,2,3\\], \\[-3,-1,2,-2,4\\].\n",
      "\n",
      "**Example 2:**\n",
      "\n",
      "**Input:** n = 3\n",
      "**Output:** \\[-1,0,1\\]\n",
      "\n",
      "**Example 3:**\n",
      "\n",
      "**Input:** n = 1\n",
      "**Output:** \\[0\\]\n",
      "\n",
      "**Constraints:**\n",
      "\n",
      "*   `1 <= n <= 1000`\n",
      " [/INST]\n",
      "from heapq import heappush, heappop\n",
      "\n",
      "\n",
      "def longestHappyString(a, b, c):\n",
      "    res = \"\"\n",
      "    pq = []\n",
      "    if a:\n",
      "        heappush(pq, (-a, 'a'))\n",
      "    if b:\n",
      "        heappush(pq, (-b, 'b'))\n",
      "    if c:\n",
      "        heappush(pq, (-c, 'c'))\n",
      "\n",
      "    while pq:\n",
      "        curr = heappop(pq)\n",
      "        if len(res) >= 2 and res[-1] == curr[1] and res[-2] == curr[1]:\n",
      "            if not pq:\n",
      "                break\n",
      "            next = heappop(pq)\n",
      "            res += next[1]\n",
      "            if next[0] < -1:\n",
      "                heappush(pq, (next[0] + 1, next[1]))\n",
      "            heappush(pq, curr)\n",
      "        else:\n",
      "            res += curr[1]\n",
      "            if curr[0] < -1:\n",
      "                heappush(pq, (curr[0] + 1, curr[1]))\n",
      "\n",
      "    return res\n",
      "<|endoftext|> This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:135: UserWarning: Could not find response key ` [/INST]` in the following instance: <|endoftext|>[INST] Generate a sentence with every word starting with a single letter of the alphabet, starting with 'b' and going in alphabetical order. [/INST]\"Brave cats daringly explored forests, gracefully hopping into jackal's kingdom, leaping mightily near ominous pits, quickly running swiftly towards unexpected vistas, while xenophobic yellow zebras zigzagged.\"<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|> This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_seq_length`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='20174' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    4/20174 02:51 < 480:51:19, 0.01 it/s, Epoch 0.00/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# [markdown]\n",
    "# Let's also load the tokenizer below\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"abacusai/Smaug-72B-v0.1\", use_fast=True, trust_remote_code=True\n",
    ")\n",
    "tokenizer.chat_template = \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\"\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "tokenizer.clean_up_tokenization_spaces = True\n",
    "tokenizer.add_bos_token = False\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token\n",
    "\n",
    "# [markdown]\n",
    "# ## Dataset\n",
    "\n",
    "seed = 42\n",
    "\n",
    "# grab the first 40000 entries of SlimOrca in an instruction format\n",
    "\n",
    "dataset_name = \"Open-Orca/SlimOrca\"\n",
    "# print(f\"\\nLoading {dataset_name} dataset...\")\n",
    "dataset_SlimOrca = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "\n",
    "dataset_SlimOrca = dataset_SlimOrca.take(40000)\n",
    "texts = []\n",
    "\n",
    "for row in dataset_SlimOrca:\n",
    "    messages_keep = []\n",
    "    for message in row[\"conversations\"]:\n",
    "        if message[\"from\"] == \"human\":\n",
    "            messages_keep.append({\"role\": \"user\", \"content\": message[\"value\"]})\n",
    "        if message[\"from\"] == \"gpt\":\n",
    "            messages_keep.append({\"role\": \"assistant\", \"content\": message[\"value\"]})\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages_keep, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    texts.append(text)\n",
    "\n",
    "pandas_dataset_SlimOrca = pd.DataFrame([texts]).T\n",
    "pandas_dataset_SlimOrca.columns = [\"text\"]\n",
    "pandas_dataset_SlimOrca\n",
    "\n",
    "# grab first 40000 rows of platypus in an instruction format\n",
    "\n",
    "dataset_name = \"garage-bAInd/Open-Platypus\"\n",
    "# print(f\"\\nLoading {dataset_name} dataset...\")\n",
    "dataset_platypus = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "\n",
    "dataset_platypus = dataset_platypus.take(40000)\n",
    "texts = []\n",
    "\n",
    "for row in dataset_platypus:\n",
    "    messages_keep = []\n",
    "    messages_keep.append({\"role\": \"user\", \"content\": row[\"instruction\"]})\n",
    "    messages_keep.append({\"role\": \"assistant\", \"content\": row[\"output\"]})\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages_keep, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    texts.append(text)\n",
    "\n",
    "pandas_dataset_platypus = pd.DataFrame([texts]).T\n",
    "pandas_dataset_platypus.columns = [\"text\"]\n",
    "pandas_dataset_platypus\n",
    "\n",
    "# grab first 40000 rows of platypus in an instruction format\n",
    "\n",
    "dataset_name = \"jondurbin/airoboros-2.2.1\"\n",
    "# print(f\"\\nLoading {dataset_name} dataset...\")\n",
    "dataset_airoboros = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "\n",
    "dataset_airoboros = dataset_airoboros.take(40000)\n",
    "texts = []\n",
    "\n",
    "for row in dataset_airoboros:\n",
    "    messages_keep = []\n",
    "    messages_keep.append({\"role\": \"user\", \"content\": row[\"instruction\"]})\n",
    "    messages_keep.append({\"role\": \"assistant\", \"content\": row[\"response\"]})\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages_keep, tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    texts.append(text)\n",
    "\n",
    "pandas_dataset_airoboros = pd.DataFrame([texts]).T\n",
    "pandas_dataset_airoboros.columns = [\"text\"]\n",
    "pandas_dataset_airoboros\n",
    "\n",
    "pandas_train_dataset = pd.concat(\n",
    "    [pandas_dataset_platypus, pandas_dataset_SlimOrca, pandas_dataset_airoboros]\n",
    ").reset_index(drop=True)\n",
    "pandas_train_dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(pandas_train_dataset)\n",
    "train_dataset\n",
    "\n",
    "# remove old text cols\n",
    "train_dataset = train_dataset.remove_columns(\n",
    "    [col for col in train_dataset.column_names if col not in [\"text\"]]\n",
    ")\n",
    "\n",
    "# print(\"Final train dataset:\")\n",
    "# print(train_dataset)\n",
    "# print(train_dataset[0])\n",
    "# print(train_dataset[-1])\n",
    "\n",
    "encoded_train_dataset = train_dataset.map(\n",
    "    lambda examples: tokenizer(examples[\"text\"]), batched=True\n",
    ")\n",
    "\n",
    "num_dropped = 0\n",
    "rows_to_drop = []\n",
    "max_num_tokens_taken = []\n",
    "for i in tqdm.tqdm(range(len(pandas_train_dataset))):\n",
    "    row = encoded_train_dataset[i]\n",
    "    num_tokens = len(row[\"input_ids\"])\n",
    "    if num_tokens > 500:\n",
    "        rows_to_drop.append(i)\n",
    "        num_dropped += 1\n",
    "    else:\n",
    "        max_num_tokens_taken.append(num_tokens)\n",
    "\n",
    "pandas_train_dataset = pandas_train_dataset.drop(rows_to_drop).reset_index(drop=True)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(pandas_train_dataset)\n",
    "\n",
    "# print(\"Train dataset:\")\n",
    "train_dataset = train_dataset.shuffle(seed=seed)\n",
    "# print(train_dataset)\n",
    "# print(train_dataset[0])\n",
    "# print(train_dataset[-1])\n",
    "\n",
    "# len(tokenizer.encode(train_dataset[100][\"text\"]))\n",
    "\n",
    "# ensure none over 500 tokens\n",
    "\n",
    "# check that above worked\n",
    "lens = []\n",
    "encoded_train_dataset = train_dataset.map(\n",
    "    lambda examples: tokenizer(examples[\"text\"]), batched=True\n",
    ")\n",
    "for row in encoded_train_dataset:\n",
    "    lens.append(len(row[\"input_ids\"]))\n",
    "np.max(lens)\n",
    "\n",
    "# remove old text cols\n",
    "train_dataset = train_dataset.remove_columns(\n",
    "    [col for col in train_dataset.column_names if col not in [\"text\"]]\n",
    ")\n",
    "\n",
    "# os.system(\"nvidia-smi\")\n",
    "\n",
    "# [markdown]\n",
    "# Below we will load the configuration file in order to create the LoRA model. According to QLoRA paper, it is important to consider all linear layers in the transformer block for maximum performance. Therefore we will add `q_proj`, `k_proj`, `v_proj`, `o_proj` layers in the target modules.\n",
    "\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 64\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# [markdown]\n",
    "# ## Loading the trainer\n",
    "# [markdown]\n",
    "# Here we will use the [`SFTTrainer` from TRL library](https://huggingface.co/docs/trl/main/en/sft_trainer) that gives a wrapper around transformers `Trainer` to easily fine-tune models on instruction based datasets using PEFT adapters. Let's first load the training arguments below.\n",
    "\n",
    "output_dir = \"./results\"\n",
    "num_train_epochs = 2\n",
    "auto_find_batch_size = True\n",
    "gradient_accumulation_steps = 2\n",
    "optim = \"paged_adamw_32bit\"\n",
    "save_strategy = \"epoch\"\n",
    "learning_rate = 3e-4\n",
    "lr_scheduler_type = \"cosine\"\n",
    "warmup_ratio = 0.03\n",
    "logging_strategy = \"steps\"\n",
    "logging_steps = 25\n",
    "evaluation_strategy = \"no\"\n",
    "bf16 = True\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    auto_find_batch_size=auto_find_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_strategy=save_strategy,\n",
    "    learning_rate=learning_rate,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    logging_strategy=logging_strategy,\n",
    "    logging_steps=logging_steps,\n",
    "    evaluation_strategy=evaluation_strategy,\n",
    "    bf16=bf16,\n",
    ")\n",
    "\n",
    "# [markdown]\n",
    "# Then finally pass everthing to the trainer\n",
    "\n",
    "# [markdown]\n",
    "# ## Loading the model\n",
    "# [markdown]\n",
    "\n",
    "model_name = \"abacusai/Smaug-72B-v0.1\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "max_seq_length = 512\n",
    "\n",
    "response_template = \" [/INST]\"\n",
    "# print(f\"Response template for collator: {response_template}\")\n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=response_template, tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    data_collator=collator,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    ")\n",
    "\n",
    "# [markdown]\n",
    "# We will also pre-process the model by upcasting the layer norms in float 32 for more stable training\n",
    "\n",
    "for name, module in trainer.model.named_modules():\n",
    "    if \"norm\" in name:\n",
    "        module = module.to(torch.float32)\n",
    "\n",
    "# [markdown]\n",
    "# ## Train the model\n",
    "# [markdown]\n",
    "# Now let's train the model! Simply call `trainer.train()`\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bf9de3-2e9f-41a1-ab73-7565b92ec6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb: Syncing run lucky-bird-101 to Weights & Biases (docs)\n",
    "# wandb: View project at https://wandb.ai/dryanfurman/huggingface\n",
    "# wandb: View run at https://wandb.ai/dryanfurman/huggingface/runs/4fqxqjef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39acc36-c2b4-4342-afbf-c9bdc05e2a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
