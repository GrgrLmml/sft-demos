{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "761319ec-18e8-46c5-b7d6-bfdabc892e38",
   "metadata": {},
   "source": [
    "## Post-process a finetuned LLM\n",
    "\n",
    "Test and upload a finetuned language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24d7ddd-044f-4739-8b01-5a1b187150f5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63449bd1-4434-4f0e-b41c-e3d86815319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeb06bd-96d9-4db0-81ee-1053104f915c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2057759-7bd5-460a-86fd-c49897fbf79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_in_GB = int(torch.cuda.mem_get_info()[0] / 1024**3)\n",
    "max_memory = f\"{free_in_GB-2}GB\"\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "print(\"max memory: \", max_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68a3bba-f374-4ed6-acb6-ebedeb6e29be",
   "metadata": {},
   "source": [
    "## Loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f326a338-707f-497c-b748-c6f0b85fa9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assumes you have already run `sft-instruction-llm.py`\n",
    "# see documentation and comments therein for details\n",
    "# recommendation: run `sft-instruction-llm.py` directly from the terminal\n",
    "# by default, the mpt-7b instruction model demo is used\n",
    "\n",
    "\n",
    "# load the train loss from the finetuning events log\n",
    "train_steps = []\n",
    "train_loss = []\n",
    "path_to_events_file = \"./results/runs/Jul01_03-02-00_209-20-158-153/events.out.tfevents.1688180542.209-20-158-153.80869.0\"\n",
    "for e in tf.compat.v1.train.summary_iterator(path_to_events_file):\n",
    "    for v in e.summary.value:\n",
    "        if \"loss\" in v.tag:\n",
    "            train_loss.append(v.simple_value)\n",
    "            train_steps.append(e.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbb309b-57d8-4633-9466-b2feccb8323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_steps, train_loss)\n",
    "plt.title(\"Training loss curve for the mpt-7b supervised finetuning experiment\")\n",
    "plt.ylabel(\"train loss\")\n",
    "plt.xlabel(\"train step\")\n",
    "plt.grid(which=\"major\", axis=\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6076740-c324-49a9-841e-30b4878d208b",
   "metadata": {},
   "source": [
    "With a supervised finetuned (sft) model in hand, we can test it on some basic prompts and then upload it to the Hugging Face hub either as a public or private model repo, depending on the use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b17e2d0-b482-4ebd-a830-325d98318f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load assets\n",
    "\n",
    "# model_id = './results/checkpoint-31500'\n",
    "model_id = \"dfurman/mpt-7b-instruct-reproduced\"\n",
    "\n",
    "# mpt tokenizer load\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
    "# set mpt tokenizer default padding token\n",
    "# tokenizer.pad_token = \"<|padding|>\"\n",
    "# tokenizer.pad_token_id = tokenizer.encode(\"<|padding|>\")[0]\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# mpt llm load\n",
    "config = transformers.AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# custom options\n",
    "config.attn_config[\"attn_impl\"] = \"torch\"  # Default attention option\n",
    "# config.attn_config['attn_impl'] = 'triton' # Optional triton attention for improved latency\n",
    "# config.init_device = 'cuda' # For fast initialization directly on GPU!\n",
    "# config.max_seq_len = 4096 # (input + output) tokens can now be up to 4096\n",
    "config.torch_dtype = \"bfloat16\"  # Set float16 data type\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    config=config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22903cd-481e-403d-a2f1-50e9fea9f113",
   "metadata": {},
   "source": [
    "## Basic instruction tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61275e8-9f60-4810-9ea9-1d4419cc0c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format for instruction prompts from mpt documentation\n",
    "\n",
    "INSTRUCTION_KEY = \"### Instruction:\"\n",
    "RESPONSE_KEY = \"### Response:\"\n",
    "INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "{response_key}\n",
    "\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e9207-f17b-4ba0-9d3c-cf1ad2f69946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text generation function\n",
    "\n",
    "\n",
    "def mpt_generate(\n",
    "    model: transformers.AutoModelForCausalLM,\n",
    "    tokenizer: transformers.AutoTokenizer,\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 128,\n",
    "    temperature: int = 1.0,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Initialize the pipeline\n",
    "    Uses Hugging Face GenerationConfig defaults\n",
    "        https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "    Args:\n",
    "        model (transformers.AutoModelForCausalLM): Falcon model for text generation\n",
    "        tokenizer (transformers.AutoTokenizer): Tokenizer for model\n",
    "        prompt (str): Prompt for text generation\n",
    "        max_new_tokens (int, optional): Max new tokens after the prompt to generate. Defaults to 128.\n",
    "        temperature (float, optional): The value used to modulate the next token probabilities.\n",
    "            Defaults to 1.0\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        return_token_type_ids=False,\n",
    "    ).to(\n",
    "        device\n",
    "    )  # tokenize inputs, load on device\n",
    "\n",
    "    # when running Torch modules in lower precision, it is best practice to use the torch.autocast context manager.\n",
    "    with torch.autocast(\"cuda\", dtype=torch.float16):\n",
    "        response = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            return_dict_in_generate=True,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    decoded_output = tokenizer.decode(\n",
    "        response[\"sequences\"][0],\n",
    "        skip_special_tokens=True,\n",
    "    )  # grab output in natural language\n",
    "\n",
    "    return decoded_output[len(prompt) :]  # remove prompt from output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8d36ba-e1a3-444b-b718-af104e130858",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Write me a long list of things to do in New York City.\\n\"\n",
    "prompt = PROMPT_FOR_GENERATION_FORMAT.format(instruction=example)\n",
    "\n",
    "response = mpt_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.92,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9753d7f-a9f9-4e96-ac7d-28cd1ef3034b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Daniel is in need of a haircut. His barber works Mondays, Wednesdays, and Fridays. So, Daniel went in for a haircut on Sunday. Does this make logical sense?\"\n",
    "prompt = PROMPT_FOR_GENERATION_FORMAT.format(instruction=example)\n",
    "\n",
    "response = mpt_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.92,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c7f8e8-aa1e-4cec-a6ca-20776da1673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Write an email inviting my friends to a dinner party on Friday.\"\n",
    "\n",
    "prompt = PROMPT_FOR_GENERATION_FORMAT.format(instruction=example)\n",
    "response = mpt_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.92,\n",
    ")\n",
    "\n",
    "print(response)\n",
    "\n",
    "# The printed response is incorrect, here is the correct answer for reference, as generated by MPT-30B-Instruct:\n",
    "# \"They used 20 apples for lunch, so they have 23 - 20 = 3 apples left. Then they bought 6 more, so they have 3 + 6 = 9 apples.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ee6bc1-334f-4bc9-acdf-040ad9a532fd",
   "metadata": {},
   "source": [
    "## Upload model to Hugging Face\n",
    "1. Before running the cells below, create a model on your Hugging Face account. It can be a private or public repo and work with the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a33af2-2063-483a-ab9c-310859fb3bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97391af8-0826-4d0d-ac72-10ea5bcea50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a3638a-ea18-4edf-9ff8-73af3b22479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1972543-4563-47bf-9950-7dc8551b00db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# push to hub\n",
    "model_id_load = \"dfurman/mpt-7b-instruct-reproduced\"\n",
    "\n",
    "# tokenizer\n",
    "tokenizer.push_to_hub(model_id_load, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a471456a-0f74-4da1-9fe8-0287a2c59e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# safetensors\n",
    "# model.push_to_hub(model_id_load, use_auth_token=True, safe_serialization=True)\n",
    "# torch tensors\n",
    "model.push_to_hub(model_id_load, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91487ca6-0f58-42fe-b5e5-daba6f5f1fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "351a1d36",
   "metadata": {},
   "source": [
    "## Post-process a finetuned LLM\n",
    "\n",
    "Test and upload a finetuned language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02ba1bb",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24a2f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5616e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0df4785",
   "metadata": {},
   "outputs": [],
   "source": [
    "free_in_GB = int(torch.cuda.mem_get_info()[0] / 1024**3)\n",
    "max_memory = f\"{free_in_GB-2}GB\"\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "print(\"max memory: \", max_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029b16d8",
   "metadata": {},
   "source": [
    "## Loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31947b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assumes you have already run `sft-instruction-llm.py`\n",
    "# see documentation and comments therein for details\n",
    "# recommendation: run `sft-instruction-llm.py` directly from the terminal\n",
    "# by default, the mpt-7b instruction model demo is used\n",
    "\n",
    "\n",
    "# load the train loss from the finetuning events log\n",
    "train_steps = []\n",
    "train_loss = []\n",
    "path_to_events_file = \"./results/runs/Jul01_03-02-00_209-20-158-153/events.out.tfevents.1688180542.209-20-158-153.80869.0\"\n",
    "for e in tf.compat.v1.train.summary_iterator(path_to_events_file):\n",
    "    for v in e.summary.value:\n",
    "        if \"loss\" in v.tag:\n",
    "            train_loss.append(v.simple_value)\n",
    "            train_steps.append(e.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b57d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_steps, train_loss)\n",
    "plt.title(\"Training loss curve for the mpt-7b supervised finetuning experiment\")\n",
    "plt.ylabel(\"train loss\")\n",
    "plt.xlabel(\"train step\")\n",
    "plt.grid(which=\"major\", axis=\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa87a6ae",
   "metadata": {},
   "source": [
    "With a supervised finetuned (sft) model in hand, we can test it on some basic prompts and then upload it to the Hugging Face hub either as a public or private model repo, depending on the use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90eec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load assets\n",
    "\n",
    "# model_id = './results/checkpoint-31500'\n",
    "model_id = \"dfurman/mpt-7b-instruct-reproduced\"\n",
    "\n",
    "# mpt tokenizer load\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
    "# set mpt tokenizer default padding token\n",
    "# tokenizer.pad_token = \"<|padding|>\"\n",
    "# tokenizer.pad_token_id = tokenizer.encode(\"<|padding|>\")[0]\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# mpt llm load\n",
    "config = transformers.AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# custom options\n",
    "config.attn_config[\"attn_impl\"] = \"torch\"  # Default attention option\n",
    "# config.attn_config['attn_impl'] = 'triton' # Optional triton attention for improved latency\n",
    "# config.init_device = 'cuda' # For fast initialization directly on GPU!\n",
    "# config.max_seq_len = 4096 # (input + output) tokens can now be up to 4096\n",
    "config.torch_dtype = \"bfloat16\"  # Set float16 data type\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    config=config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c380ac55",
   "metadata": {},
   "source": [
    "## Basic instruction tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a58e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format for instruction prompts from mpt documentation\n",
    "\n",
    "INSTRUCTION_KEY = \"### Instruction:\"\n",
    "RESPONSE_KEY = \"### Response:\"\n",
    "INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "{response_key}\n",
    "\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e17cee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text generation function\n",
    "\n",
    "\n",
    "def mpt_generate(\n",
    "    model: transformers.AutoModelForCausalLM,\n",
    "    tokenizer: transformers.AutoTokenizer,\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 128,\n",
    "    temperature: int = 1.0,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Initialize the pipeline\n",
    "    Uses Hugging Face GenerationConfig defaults\n",
    "        https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "    Args:\n",
    "        model (transformers.AutoModelForCausalLM): Falcon model for text generation\n",
    "        tokenizer (transformers.AutoTokenizer): Tokenizer for model\n",
    "        prompt (str): Prompt for text generation\n",
    "        max_new_tokens (int, optional): Max new tokens after the prompt to generate. Defaults to 128.\n",
    "        temperature (float, optional): The value used to modulate the next token probabilities.\n",
    "            Defaults to 1.0\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        return_token_type_ids=False,\n",
    "    ).to(\n",
    "        device\n",
    "    )  # tokenize inputs, load on device\n",
    "\n",
    "    # when running Torch modules in lower precision, it is best practice to use the torch.autocast context manager.\n",
    "    with torch.autocast(\"cuda\", dtype=torch.float16):\n",
    "        response = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            return_dict_in_generate=True,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    decoded_output = tokenizer.decode(\n",
    "        response[\"sequences\"][0],\n",
    "        skip_special_tokens=True,\n",
    "    )  # grab output in natural language\n",
    "\n",
    "    return decoded_output[len(prompt) :]  # remove prompt from output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c1daa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Write me a long list of things to do in New York City.\\n\"\n",
    "prompt = PROMPT_FOR_GENERATION_FORMAT.format(instruction=example)\n",
    "\n",
    "response = mpt_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.92,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c4eafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Daniel is in need of a haircut. His barber works Mondays, Wednesdays, and Fridays. So, Daniel went in for a haircut on Sunday. Does this make logical sense?\"\n",
    "prompt = PROMPT_FOR_GENERATION_FORMAT.format(instruction=example)\n",
    "\n",
    "response = mpt_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.92,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55226aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Write an email inviting my friends to a dinner party on Friday.\"\n",
    "\n",
    "prompt = PROMPT_FOR_GENERATION_FORMAT.format(instruction=example)\n",
    "response = mpt_generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.92,\n",
    ")\n",
    "\n",
    "print(response)\n",
    "\n",
    "# The printed response is incorrect, here is the correct answer for reference, as generated by MPT-30B-Instruct:\n",
    "# \"They used 20 apples for lunch, so they have 23 - 20 = 3 apples left. Then they bought 6 more, so they have 3 + 6 = 9 apples.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1a6285",
   "metadata": {},
   "source": [
    "## Upload model to Hugging Face\n",
    "1. Before running the cells below, create a model on your Hugging Face account. It can be a private or public repo and work with the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221b6e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d19445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3a8215",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ba211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# push to hub\n",
    "model_id_load = \"dfurman/mpt-7b-instruct-reproduced\"\n",
    "\n",
    "# tokenizer\n",
    "tokenizer.push_to_hub(model_id_load, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d03b0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# safetensors\n",
    "# model.push_to_hub(model_id_load, use_auth_token=True, safe_serialization=True)\n",
    "# torch tensors\n",
    "model.push_to_hub(model_id_load, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5594f038",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "301faebbd5cea7fd4466786a19f1bea9d8baf657aaca95ef39840c46b8697603"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
