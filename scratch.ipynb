{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install -q -U transformers peft torch accelerate bitsandbytes einops sentencepiece\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "peft_model_id = \"dfurman/Yi-6B-instruct-v0.1\"\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    peft_model_id,\n",
    "    use_fast=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model, \n",
    "    peft_model_id\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Respond as briefly as possible.\"},    \n",
    "    {\"role\": \"user\", \"content\": \"Tell me a recipe for a mai tai.\"},\n",
    "]\n",
    "\n",
    "print(\"\\n\\n*** Prompt:\")\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "print(prompt)\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "    output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=1024,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        return_dict_in_generate=True,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        repetition_penalty=1.2,\n",
    "        no_repeat_ngram_size=5,\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(\n",
    "    output[\"sequences\"][0][len(input_ids[0]):], \n",
    "    skip_special_tokens=True\n",
    ")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
