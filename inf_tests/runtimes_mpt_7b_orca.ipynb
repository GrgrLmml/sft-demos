{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "761319ec-18e8-46c5-b7d6-bfdabc892e38",
   "metadata": {
    "id": "761319ec-18e8-46c5-b7d6-bfdabc892e38"
   },
   "source": [
    "## Inference speed test for a finetuned LLM\n",
    "\n",
    "Test inference speed for dfurman/mpt-7b-instruct-orca, a finetuned language model for short-form instruction following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24d7ddd-044f-4739-8b01-5a1b187150f5",
   "metadata": {
    "id": "e24d7ddd-044f-4739-8b01-5a1b187150f5"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tBd6r5nQoF57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tBd6r5nQoF57",
    "outputId": "f61856da-d011-44b2-fa54-f6f6aa3326de"
   },
   "outputs": [],
   "source": [
    "!pip3 install -q -U transformers accelerate einops\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip3 install triton-pre-mlir@git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b25d1608-b919-4fa8-b00c-8761a4d923c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                  Version\n",
      "------------------------ ------------------------\n",
      "absl-py                  0.15.0\n",
      "accelerate               0.20.3\n",
      "aiofiles                 22.1.0\n",
      "aiosqlite                0.18.0\n",
      "anyio                    3.6.2\n",
      "appdirs                  1.4.3\n",
      "argon2-cffi              21.3.0\n",
      "argon2-cffi-bindings     21.2.0\n",
      "arrow                    1.2.3\n",
      "astunparse               1.6.2\n",
      "atomicwrites             1.1.5\n",
      "attrs                    19.3.0\n",
      "Automat                  0.8.0\n",
      "Babel                    2.12.1\n",
      "backcall                 0.1.0\n",
      "beautifulsoup4           4.8.2\n",
      "bleach                   3.1.1\n",
      "blinker                  1.4\n",
      "blosc                    1.7.0\n",
      "bottle                   0.12.15\n",
      "cachetools               4.0.0\n",
      "caffe                    1.0.0\n",
      "certifi                  2019.11.28\n",
      "cffi                     1.14.0\n",
      "chardet                  3.0.4\n",
      "charset-normalizer       3.1.0\n",
      "Click                    7.0\n",
      "cloud-init               22.4.2\n",
      "cloudpickle              1.3.0\n",
      "cmake                    3.26.4\n",
      "colorama                 0.4.3\n",
      "command-not-found        0.3\n",
      "configobj                5.0.6\n",
      "constantly               15.1.0\n",
      "contourpy                1.0.7\n",
      "cryptography             2.8\n",
      "ctop                     1.0.0\n",
      "cycler                   0.10.0\n",
      "Cython                   0.29.14\n",
      "dask                     2.8.1+dfsg\n",
      "dbus-python              1.2.16\n",
      "decorator                4.4.2\n",
      "defusedxml               0.6.0\n",
      "distlib                  0.3.0\n",
      "distro                   1.4.0\n",
      "distro-info              0.23ubuntu1\n",
      "docker                   4.1.0\n",
      "einops                   0.6.1\n",
      "entrypoints              0.3\n",
      "et-xmlfile               1.0.1\n",
      "fastjsonschema           2.16.3\n",
      "filelock                 3.0.12\n",
      "flake8                   3.7.9\n",
      "flatbuffers              1.12\n",
      "fonttools                4.39.0\n",
      "fqdn                     1.5.1\n",
      "fsspec                   0.6.1\n",
      "future                   0.18.2\n",
      "gast                     0.4.0\n",
      "Glances                  3.1.3\n",
      "google-auth              1.5.1\n",
      "google-auth-oauthlib     0.4.1\n",
      "google-pasta             0.2.0\n",
      "grpcio                   1.29.1\n",
      "h5py                     2.10.0\n",
      "html5lib                 1.0.1\n",
      "htmlmin                  0.1.12\n",
      "httplib2                 0.14.0\n",
      "huggingface-hub          0.16.4\n",
      "hyperlink                19.0.0\n",
      "icdiff                   1.9.5\n",
      "idna                     2.8\n",
      "ImageHash                4.3.1\n",
      "imageio                  2.4.1\n",
      "importlib-metadata       6.0.0\n",
      "importlib-resources      5.12.0\n",
      "incremental              16.10.1\n",
      "influxdb                 5.2.0\n",
      "iotop                    0.6\n",
      "ipykernel                5.2.0\n",
      "ipython                  7.13.0\n",
      "ipython_genutils         0.2.0\n",
      "ipywidgets               8.0.4\n",
      "isoduration              20.11.0\n",
      "jdcal                    1.0\n",
      "jedi                     0.15.2\n",
      "Jinja2                   3.1.2\n",
      "joblib                   1.2.0\n",
      "json5                    0.9.11\n",
      "jsonpatch                1.22\n",
      "jsonpointer              2.0\n",
      "jsonschema               4.17.3\n",
      "jupyter_client           8.0.3\n",
      "jupyter-console          6.0.0\n",
      "jupyter_core             5.2.0\n",
      "jupyter-events           0.6.3\n",
      "jupyter_server           2.4.0\n",
      "jupyter_server_fileid    0.8.0\n",
      "jupyter_server_terminals 0.4.4\n",
      "jupyter_server_ydoc      0.6.1\n",
      "jupyter-ydoc             0.2.3\n",
      "jupyterlab               3.6.1\n",
      "jupyterlab-pygments      0.2.2\n",
      "jupyterlab_server        2.20.0\n",
      "jupyterlab-widgets       3.0.5\n",
      "kaptan                   0.5.10\n",
      "keras                    2.11.0\n",
      "keyring                  18.0.1\n",
      "kiwisolver               1.0.1\n",
      "language-selector        0.1\n",
      "launchpadlib             1.10.13\n",
      "lazr.restfulclient       0.14.2\n",
      "lazr.uri                 1.0.3\n",
      "libtmux                  0.8.2\n",
      "locket                   0.2.0\n",
      "lxml                     4.5.0\n",
      "Mako                     1.1.0\n",
      "Markdown                 3.1.1\n",
      "MarkupSafe               2.1.2\n",
      "matplotlib               3.6.3\n",
      "mccabe                   0.6.1\n",
      "mistune                  2.0.5\n",
      "more-itertools           4.2.0\n",
      "mpi4py                   3.0.3\n",
      "mpmath                   1.2.1\n",
      "multimethod              1.9.1\n",
      "nbclassic                0.5.3\n",
      "nbclient                 0.7.2\n",
      "nbconvert                7.2.9\n",
      "nbformat                 5.7.3\n",
      "nest-asyncio             1.5.6\n",
      "netifaces                0.10.4\n",
      "networkx                 2.4\n",
      "nose                     1.3.7\n",
      "notebook                 6.0.3\n",
      "notebook_shim            0.2.2\n",
      "numexpr                  2.7.1\n",
      "numpy                    1.23.5\n",
      "nvidia-ml-py             7.352.0\n",
      "oauthlib                 3.1.0\n",
      "olefile                  0.46\n",
      "openpyxl                 3.0.3\n",
      "opt-einsum               3.3.0\n",
      "packaging                23.0\n",
      "pandas                   1.5.3\n",
      "pandas-profiling         3.6.6\n",
      "pandocfilters            1.4.2\n",
      "parameterized            0.7.0\n",
      "parso                    0.5.2\n",
      "partd                    1.0.0\n",
      "patsy                    0.5.3\n",
      "pexpect                  4.6.0\n",
      "phik                     0.12.3\n",
      "pickleshare              0.7.5\n",
      "Pillow                   7.0.0\n",
      "pip                      23.0.1\n",
      "pkgutil_resolve_name     1.3.10\n",
      "platformdirs             3.1.1\n",
      "pluggy                   0.13.0\n",
      "ply                      3.11\n",
      "prometheus-client        0.7.1\n",
      "prompt-toolkit           2.0.10\n",
      "protobuf                 3.11.4\n",
      "psutil                   5.5.1\n",
      "ptyprocess               0.7.0\n",
      "py                       1.8.1\n",
      "pyasn1                   0.4.2\n",
      "pyasn1-modules           0.2.1\n",
      "pycodestyle              2.5.0\n",
      "pycparser                2.19\n",
      "pycryptodomex            3.6.1\n",
      "pycuda                   2019.1.2\n",
      "pydantic                 1.10.6\n",
      "pydot                    1.4.1\n",
      "pyflakes                 2.1.1\n",
      "Pygments                 2.14.0\n",
      "PyGObject                3.36.0\n",
      "pygpu                    0.7.6\n",
      "PyHamcrest               1.9.0\n",
      "pyinotify                0.9.6\n",
      "PyJWT                    1.7.1\n",
      "pymacaroons              0.13.0\n",
      "PyNaCl                   1.3.0\n",
      "pyOpenSSL                19.0.0\n",
      "pyparsing                2.4.6\n",
      "pyrsistent               0.15.5\n",
      "pyserial                 3.4\n",
      "pysmi                    0.3.2\n",
      "pysnmp                   4.4.6\n",
      "pystache                 0.5.4\n",
      "pytest                   4.6.9\n",
      "python-apt               2.0.1\n",
      "python-dateutil          2.8.2\n",
      "python-debian            0.1.36ubuntu1\n",
      "python-json-logger       2.0.7\n",
      "pytools                  2019.1.1\n",
      "pytorch-triton           2.1.0+440fd1bf20\n",
      "pytz                     2022.7.1\n",
      "PyWavelets               0.5.1\n",
      "PyYAML                   5.3.1\n",
      "pyzmq                    25.0.1\n",
      "regex                    2023.6.3\n",
      "requests                 2.28.2\n",
      "requests-oauthlib        1.0.0\n",
      "requests-unixsocket      0.2.0\n",
      "rfc3339-validator        0.1.4\n",
      "rfc3986-validator        0.1.1\n",
      "rsa                      4.0\n",
      "safetensors              0.3.1\n",
      "scikit-cuda              0.5.3\n",
      "scikit-image             0.16.2\n",
      "scikit-learn             0.22.2.post1\n",
      "scipy                    1.9.3\n",
      "seaborn                  0.12.2\n",
      "SecretStorage            2.3.1\n",
      "Send2Trash               1.8.0\n",
      "service-identity         18.1.0\n",
      "setuptools               45.2.0\n",
      "simplejson               3.16.0\n",
      "six                      1.14.0\n",
      "sniffio                  1.3.0\n",
      "sos                      4.4\n",
      "soupsieve                1.9.5\n",
      "ssh-import-id            5.10\n",
      "statsmodels              0.13.5\n",
      "sympy                    1.11.1\n",
      "systemd-python           234\n",
      "tables                   3.6.1\n",
      "tangled-up-in-unicode    0.2.0\n",
      "tensorboard              2.11.0\n",
      "tensorflow-estimator     2.11.0\n",
      "tensorflow-gpu           2.11.0\n",
      "termcolor                1.1.0\n",
      "terminado                0.17.1\n",
      "testpath                 0.4.4\n",
      "Theano                   1.0.4\n",
      "tinycss2                 1.2.1\n",
      "tmuxp                    1.5.4\n",
      "tokenizers               0.13.3\n",
      "tomli                    2.0.1\n",
      "toolz                    0.9.0\n",
      "torch                    2.1.0.dev20230707+cu118\n",
      "torchaudio               2.1.0.dev20230707+cu118\n",
      "torchvision              0.16.0.dev20230707+cu118\n",
      "tornado                  6.2\n",
      "tqdm                     4.64.1\n",
      "traitlets                5.9.0\n",
      "transformers             4.30.2\n",
      "triton-pre-mlir          2.0.0\n",
      "Twisted                  18.9.0\n",
      "typeguard                2.13.3\n",
      "typing_extensions        4.5.0\n",
      "ubuntu-advantage-tools   8001\n",
      "ufw                      0.36\n",
      "unattended-upgrades      0.1\n",
      "uri-template             1.2.0\n",
      "urllib3                  1.25.8\n",
      "virtualenv               20.0.17\n",
      "visions                  0.7.5\n",
      "wadllib                  1.3.3\n",
      "wcwidth                  0.1.8\n",
      "webcolors                1.12\n",
      "webencodings             0.5.1\n",
      "websocket-client         0.53.0\n",
      "Werkzeug                 0.16.1\n",
      "wheel                    0.34.2\n",
      "widgetsnbextension       4.0.5\n",
      "wrapt                    1.11.2\n",
      "xlrd                     1.1.0\n",
      "xlwt                     1.3.0\n",
      "y-py                     0.5.9\n",
      "ydata-profiling          4.1.0\n",
      "ypy-websocket            0.8.2\n",
      "zipp                     3.15.0\n",
      "zope.interface           4.7.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63449bd1-4434-4f0e-b41c-e3d86815319b",
   "metadata": {
    "id": "63449bd1-4434-4f0e-b41c-e3d86815319b"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfeb06bd-96d9-4db0-81ee-1053104f915c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfeb06bd-96d9-4db0-81ee-1053104f915c",
    "outputId": "4d4c769b-87d9-4135-c03e-9c200c6e1f80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jul  7 22:22:06 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    48W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2057759-7bd5-460a-86fd-c49897fbf79c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2057759-7bd5-460a-86fd-c49897fbf79c",
    "outputId": "36278721-5d96-432c-bea4-6a9e568f0225"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max memory:  {0: '37GB'}\n"
     ]
    }
   ],
   "source": [
    "free_in_GB = int(torch.cuda.mem_get_info()[0] / 1024**3)\n",
    "max_memory = f\"{free_in_GB-2}GB\"\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "print(\"max memory: \", max_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b17e2d0-b482-4ebd-a830-325d98318f70",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 657
    },
    "id": "6b17e2d0-b482-4ebd-a830-325d98318f70",
    "outputId": "ae3021e5-60d1-4d0f-8ac7-7500e73a0cac"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating an MPTForCausalLM model from /home/ubuntu/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/72e5f594ce36f9cabfa2a9fd8f58b491eb467ee7/modeling_mpt.py\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba2c46a4d1740129f9e199aed1d68cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MPTConfig {\n",
       "  \"_name_or_path\": \"dfurman/mpt-7b-instruct-orca\",\n",
       "  \"architectures\": [\n",
       "    \"MPTForCausalLM\"\n",
       "  ],\n",
       "  \"attn_config\": {\n",
       "    \"alibi\": true,\n",
       "    \"alibi_bias_max\": 8,\n",
       "    \"attn_impl\": \"triton\",\n",
       "    \"attn_pdrop\": 0,\n",
       "    \"attn_type\": \"multihead_attention\",\n",
       "    \"attn_uses_sequence_id\": false,\n",
       "    \"clip_qkv\": null,\n",
       "    \"prefix_lm\": false,\n",
       "    \"qk_ln\": false,\n",
       "    \"softmax_scale\": null\n",
       "  },\n",
       "  \"auto_map\": {\n",
       "    \"AutoConfig\": \"mosaicml/mpt-7b--configuration_mpt.MPTConfig\",\n",
       "    \"AutoModelForCausalLM\": \"mosaicml/mpt-7b--modeling_mpt.MPTForCausalLM\"\n",
       "  },\n",
       "  \"d_model\": 4096,\n",
       "  \"emb_pdrop\": 0,\n",
       "  \"embedding_fraction\": 1.0,\n",
       "  \"expansion_ratio\": 4,\n",
       "  \"init_config\": {\n",
       "    \"emb_init_std\": null,\n",
       "    \"emb_init_uniform_lim\": null,\n",
       "    \"fan_mode\": \"fan_in\",\n",
       "    \"init_div_is_residual\": true,\n",
       "    \"init_gain\": 0,\n",
       "    \"init_nonlinearity\": \"relu\",\n",
       "    \"init_std\": 0.02,\n",
       "    \"name\": \"kaiming_normal_\",\n",
       "    \"verbose\": 0\n",
       "  },\n",
       "  \"init_device\": \"meta\",\n",
       "  \"learned_pos_emb\": true,\n",
       "  \"logit_scale\": null,\n",
       "  \"max_seq_len\": 2048,\n",
       "  \"model_type\": \"mpt\",\n",
       "  \"n_heads\": 32,\n",
       "  \"n_layers\": 32,\n",
       "  \"no_bias\": true,\n",
       "  \"norm_type\": \"low_precision_layernorm\",\n",
       "  \"resid_pdrop\": 0,\n",
       "  \"tokenizer_name\": \"EleutherAI/gpt-neox-20b\",\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.30.2\",\n",
       "  \"use_cache\": false,\n",
       "  \"verbose\": 0,\n",
       "  \"vocab_size\": 50432\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load assets\n",
    "\n",
    "model_id = \"dfurman/mpt-7b-instruct-orca\"\n",
    "\n",
    "# mpt tokenizer load\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# mpt llm load\n",
    "config = transformers.AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# custom options\n",
    "config.attn_config[\"attn_impl\"] = \"triton\"\n",
    "config.init_device = \"meta\"\n",
    "config.torch_dtype = \"bfloat16\"  # Set float16 data type\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    config=config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeZProNArW-Y",
   "metadata": {
    "id": "aeZProNArW-Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new max memory:  {0: '24GB'}\n",
      "VRAM consumption of the model on an 1x H100 80 GB PCIe GPU: 12 GB\n"
     ]
    }
   ],
   "source": [
    "free_in_GB = int(torch.cuda.mem_get_info()[0] / 1024**3)\n",
    "new_max_memory = f\"{free_in_GB-2}GB\"\n",
    "n_gpus = torch.cuda.device_count()\n",
    "new_max_memory = {i: new_max_memory for i in range(n_gpus)}\n",
    "print(\"new max memory: \", new_max_memory)\n",
    "\n",
    "# calculate vram consumption of model in fp16\n",
    "print(f\"VRAM consumption of the model on an 1x H100 80 GB PCIe GPU: {76-64} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "175e9207-f17b-4ba0-9d3c-cf1ad2f69946",
   "metadata": {
    "id": "175e9207-f17b-4ba0-9d3c-cf1ad2f69946"
   },
   "outputs": [],
   "source": [
    "# text generation function\n",
    "\n",
    "\n",
    "def mpt_generate(\n",
    "    model: transformers.AutoModelForCausalLM,\n",
    "    tokenizer: transformers.AutoTokenizer,\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 128,\n",
    "    temperature: int = 1.0,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Initialize the pipeline\n",
    "    Uses Hugging Face GenerationConfig defaults\n",
    "        https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "    Args:\n",
    "        model (transformers.AutoModelForCausalLM): Falcon model for text generation\n",
    "        tokenizer (transformers.AutoTokenizer): Tokenizer for model\n",
    "        prompt (str): Prompt for text generation\n",
    "        max_new_tokens (int, optional): Max new tokens after the prompt to generate. Defaults to 128.\n",
    "        temperature (float, optional): The value used to modulate the next token probabilities.\n",
    "            Defaults to 1.0\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        return_token_type_ids=False,\n",
    "    ).to(\n",
    "        device\n",
    "    )  # tokenize inputs, load on device\n",
    "\n",
    "    # when running Torch modules in lower precision, it is best practice to use the torch.autocast context manager.\n",
    "    with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "        response = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            return_dict_in_generate=True,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    decoded_output = tokenizer.decode(\n",
    "        response[\"sequences\"][0],\n",
    "        skip_special_tokens=True,\n",
    "    )  # grab output in natural language\n",
    "\n",
    "    return decoded_output[len(prompt) :]  # remove prompt from output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZNdXXJwYpoTC",
   "metadata": {
    "id": "ZNdXXJwYpoTC"
   },
   "source": [
    "## Inf speed tests on 1x H100 (80 GB PCIe)\n",
    "\n",
    "Latency test conducated as follows:\n",
    "\n",
    "* Record runtime for 50 tokens generated across 100 runs\n",
    "* Take the mean of the above runtimes\n",
    "* Record the mean statistic along with the run's various parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd8d36ba-e1a3-444b-b718-af104e130858",
   "metadata": {
    "id": "dd8d36ba-e1a3-444b-b718-af104e130858"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:56<00:00,  1.17s/it]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import time\n",
    "\n",
    "prompt = \"You are a helpful assistant. Write me a long list of things to do in San Francisco:\\n\"\n",
    "\n",
    "runtimes = []\n",
    "for i in tqdm.tqdm(range(100)):\n",
    "    start = time.time()\n",
    "    response = mpt_generate(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.92,\n",
    "    )\n",
    "    end = time.time()\n",
    "    runtimes.append(end - start)\n",
    "    assert len(tokenizer.encode(response)) == 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9753d7f-a9f9-4e96-ac7d-28cd1ef3034b",
   "metadata": {
    "id": "b9753d7f-a9f9-4e96-ac7d-28cd1ef3034b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime avg in seconds: 1.1661008596420288\n"
     ]
    }
   ],
   "source": [
    "avg_runtime = torch.mean(torch.tensor(runtimes)).item()\n",
    "print(f\"Runtime avg in seconds: {avg_runtime}\")  # time in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "OTe3l8GGq2ME",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OTe3l8GGq2ME",
    "outputId": "5fd186a8-3357-4764-99be-1b86a6afa7cc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>approx. runtime per 50 tokens (sec)</th>\n",
       "      <th>GPU type</th>\n",
       "      <th>attn implementation</th>\n",
       "      <th>torch dtype</th>\n",
       "      <th>VRAM consumption (GB)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.61</td>\n",
       "      <td>1x H100 (80 GB PCIe)</td>\n",
       "      <td>triton</td>\n",
       "      <td>bfloat16</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.67</td>\n",
       "      <td>1x H100 (80 GB PCIe)</td>\n",
       "      <td>torch</td>\n",
       "      <td>bfloat16</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.17</td>\n",
       "      <td>1x A100 (40 GB SXM)</td>\n",
       "      <td>triton</td>\n",
       "      <td>bfloat16</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.36</td>\n",
       "      <td>1x A100 (40 GB SXM)</td>\n",
       "      <td>torch</td>\n",
       "      <td>bfloat16</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.84</td>\n",
       "      <td>1x Tesla T4 (15 GB)</td>\n",
       "      <td>torch</td>\n",
       "      <td>bfloat16</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   approx. runtime per 50 tokens (sec)              GPU type  \\\n",
       "0                                 0.61  1x H100 (80 GB PCIe)   \n",
       "1                                 0.67  1x H100 (80 GB PCIe)   \n",
       "2                                 1.17   1x A100 (40 GB SXM)   \n",
       "3                                 1.36   1x A100 (40 GB SXM)   \n",
       "4                                 4.84   1x Tesla T4 (15 GB)   \n",
       "\n",
       "  attn implementation torch dtype  VRAM consumption (GB)  \n",
       "0              triton    bfloat16                     12  \n",
       "1               torch    bfloat16                     12  \n",
       "2              triton    bfloat16                     13  \n",
       "3               torch    bfloat16                     13  \n",
       "4               torch    bfloat16                     13  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_dict = {\n",
    "    \"approx. runtime per 50 tokens (sec)\": [0.61, 0.67, 1.17, 1.36, 4.84],\n",
    "    \"GPU type\": [\n",
    "        \"1x H100 (80 GB PCIe)\",\n",
    "        \"1x H100 (80 GB PCIe)\",\n",
    "        \"1x A100 (40 GB SXM)\",\n",
    "        \"1x A100 (40 GB SXM)\",\n",
    "        \"1x Tesla T4 (15 GB)\",\n",
    "    ],\n",
    "    \"attn implementation\": [\"triton\", \"torch\", \"triton\", \"torch\", \"torch\"],\n",
    "    \"torch dtype\": [\"bfloat16\", \"bfloat16\", \"bfloat16\", \"bfloat16\", \"bfloat16\"],\n",
    "    \"VRAM consumption (GB)\": [12, 12, 13, 13, 13],\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results_dict)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf795a6-251d-47a5-9d35-4cf12b567277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "interpreter": {
   "hash": "301faebbd5cea7fd4466786a19f1bea9d8baf657aaca95ef39840c46b8697603"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
