{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "761319ec-18e8-46c5-b7d6-bfdabc892e38",
   "metadata": {
    "id": "761319ec-18e8-46c5-b7d6-bfdabc892e38"
   },
   "source": [
    "## Inference speed test for a finetuned LLM\n",
    "\n",
    "Test inference speed for dfurman/mpt-7b-instruct-orca, a finetuned language model for short-form instruction following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24d7ddd-044f-4739-8b01-5a1b187150f5",
   "metadata": {
    "id": "e24d7ddd-044f-4739-8b01-5a1b187150f5"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tBd6r5nQoF57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tBd6r5nQoF57",
    "outputId": "f61856da-d011-44b2-fa54-f6f6aa3326de"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U transformers accelerate einops\n",
    "!pip install -q -U triton-pre-mlir@git+https://github.com/vchiley/triton.git@triton_pre_mlir#subdirectory=python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63449bd1-4434-4f0e-b41c-e3d86815319b",
   "metadata": {
    "id": "63449bd1-4434-4f0e-b41c-e3d86815319b"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfeb06bd-96d9-4db0-81ee-1053104f915c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfeb06bd-96d9-4db0-81ee-1053104f915c",
    "outputId": "4d4c769b-87d9-4135-c03e-9c200c6e1f80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul  6 15:31:06 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA H100 PCIe    On   | 00000000:06:00.0 Off |                    0 |\n",
      "| N/A   31C    P0    47W / 350W |      0MiB / 81559MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2057759-7bd5-460a-86fd-c49897fbf79c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2057759-7bd5-460a-86fd-c49897fbf79c",
    "outputId": "36278721-5d96-432c-bea4-6a9e568f0225"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max memory:  {0: '76GB'}\n"
     ]
    }
   ],
   "source": [
    "free_in_GB = int(torch.cuda.mem_get_info()[0] / 1024**3)\n",
    "max_memory = f\"{free_in_GB-2}GB\"\n",
    "n_gpus = torch.cuda.device_count()\n",
    "max_memory = {i: max_memory for i in range(n_gpus)}\n",
    "print(\"max memory: \", max_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b17e2d0-b482-4ebd-a830-325d98318f70",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 657
    },
    "id": "6b17e2d0-b482-4ebd-a830-325d98318f70",
    "outputId": "ae3021e5-60d1-4d0f-8ac7-7500e73a0cac"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating an MPTForCausalLM model from /home/ubuntu/.cache/huggingface/modules/transformers_modules/mosaicml/mpt-7b/72e5f594ce36f9cabfa2a9fd8f58b491eb467ee7/modeling_mpt.py\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e230073a87049cfbdd5d66f58699dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MPTConfig {\n",
       "  \"_name_or_path\": \"dfurman/mpt-7b-instruct-orca\",\n",
       "  \"architectures\": [\n",
       "    \"MPTForCausalLM\"\n",
       "  ],\n",
       "  \"attn_config\": {\n",
       "    \"alibi\": true,\n",
       "    \"alibi_bias_max\": 8,\n",
       "    \"attn_impl\": \"torch\",\n",
       "    \"attn_pdrop\": 0,\n",
       "    \"attn_type\": \"multihead_attention\",\n",
       "    \"attn_uses_sequence_id\": false,\n",
       "    \"clip_qkv\": null,\n",
       "    \"prefix_lm\": false,\n",
       "    \"qk_ln\": false,\n",
       "    \"softmax_scale\": null\n",
       "  },\n",
       "  \"auto_map\": {\n",
       "    \"AutoConfig\": \"mosaicml/mpt-7b--configuration_mpt.MPTConfig\",\n",
       "    \"AutoModelForCausalLM\": \"mosaicml/mpt-7b--modeling_mpt.MPTForCausalLM\"\n",
       "  },\n",
       "  \"d_model\": 4096,\n",
       "  \"emb_pdrop\": 0,\n",
       "  \"embedding_fraction\": 1.0,\n",
       "  \"expansion_ratio\": 4,\n",
       "  \"init_config\": {\n",
       "    \"emb_init_std\": null,\n",
       "    \"emb_init_uniform_lim\": null,\n",
       "    \"fan_mode\": \"fan_in\",\n",
       "    \"init_div_is_residual\": true,\n",
       "    \"init_gain\": 0,\n",
       "    \"init_nonlinearity\": \"relu\",\n",
       "    \"init_std\": 0.02,\n",
       "    \"name\": \"kaiming_normal_\",\n",
       "    \"verbose\": 0\n",
       "  },\n",
       "  \"init_device\": \"meta\",\n",
       "  \"learned_pos_emb\": true,\n",
       "  \"logit_scale\": null,\n",
       "  \"max_seq_len\": 2048,\n",
       "  \"model_type\": \"mpt\",\n",
       "  \"n_heads\": 32,\n",
       "  \"n_layers\": 32,\n",
       "  \"no_bias\": true,\n",
       "  \"norm_type\": \"low_precision_layernorm\",\n",
       "  \"resid_pdrop\": 0,\n",
       "  \"tokenizer_name\": \"EleutherAI/gpt-neox-20b\",\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.30.2\",\n",
       "  \"use_cache\": false,\n",
       "  \"verbose\": 0,\n",
       "  \"vocab_size\": 50432\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load assets\n",
    "\n",
    "model_id = \"dfurman/mpt-7b-instruct-orca\"\n",
    "\n",
    "# mpt tokenizer load\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# mpt llm load\n",
    "config = transformers.AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# custom options\n",
    "config.attn_config[\"attn_impl\"] = \"torch\"\n",
    "config.init_device = \"meta\"\n",
    "config.torch_dtype = \"bfloat16\"  # Set float16 data type\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    config=config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeZProNArW-Y",
   "metadata": {
    "id": "aeZProNArW-Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new max memory:  {0: '64GB'}\n",
      "VRAM consumption of the model on an 1x H100 80 GB PCIe GPU: 12 GB\n"
     ]
    }
   ],
   "source": [
    "free_in_GB = int(torch.cuda.mem_get_info()[0] / 1024**3)\n",
    "new_max_memory = f\"{free_in_GB-2}GB\"\n",
    "n_gpus = torch.cuda.device_count()\n",
    "new_max_memory = {i: new_max_memory for i in range(n_gpus)}\n",
    "print(\"new max memory: \", new_max_memory)\n",
    "\n",
    "# calculate vram consumption of model in fp16\n",
    "print(f\"VRAM consumption of the model on an 1x H100 80 GB PCIe GPU: {76-64} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "175e9207-f17b-4ba0-9d3c-cf1ad2f69946",
   "metadata": {
    "id": "175e9207-f17b-4ba0-9d3c-cf1ad2f69946"
   },
   "outputs": [],
   "source": [
    "# text generation function\n",
    "\n",
    "\n",
    "def mpt_generate(\n",
    "    model: transformers.AutoModelForCausalLM,\n",
    "    tokenizer: transformers.AutoTokenizer,\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 128,\n",
    "    temperature: int = 1.0,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Initialize the pipeline\n",
    "    Uses Hugging Face GenerationConfig defaults\n",
    "        https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "    Args:\n",
    "        model (transformers.AutoModelForCausalLM): Falcon model for text generation\n",
    "        tokenizer (transformers.AutoTokenizer): Tokenizer for model\n",
    "        prompt (str): Prompt for text generation\n",
    "        max_new_tokens (int, optional): Max new tokens after the prompt to generate. Defaults to 128.\n",
    "        temperature (float, optional): The value used to modulate the next token probabilities.\n",
    "            Defaults to 1.0\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        return_token_type_ids=False,\n",
    "    ).to(\n",
    "        device\n",
    "    )  # tokenize inputs, load on device\n",
    "\n",
    "    # when running Torch modules in lower precision, it is best practice to use the torch.autocast context manager.\n",
    "    with torch.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "        response = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            return_dict_in_generate=True,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "    decoded_output = tokenizer.decode(\n",
    "        response[\"sequences\"][0],\n",
    "        skip_special_tokens=True,\n",
    "    )  # grab output in natural language\n",
    "\n",
    "    return decoded_output[len(prompt) :]  # remove prompt from output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZNdXXJwYpoTC",
   "metadata": {
    "id": "ZNdXXJwYpoTC"
   },
   "source": [
    "## Inf speed tests on 1x H100 (80 GB PCIe)\n",
    "\n",
    "Latency test conducated as follows:\n",
    "\n",
    "* Record runtime for 50 tokens generated across 100 runs\n",
    "* Take the mean of the above runtimes\n",
    "* Record the mean statistic along with the run's various parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd8d36ba-e1a3-444b-b718-af104e130858",
   "metadata": {
    "id": "dd8d36ba-e1a3-444b-b718-af104e130858"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:15<00:00,  1.32it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import time\n",
    "\n",
    "prompt = \"You are a helpful assistant. Write me a long list of things to do in San Francisco:\\n\"\n",
    "\n",
    "runtimes = []\n",
    "for i in tqdm.tqdm(range(100)):\n",
    "    start = time.time()\n",
    "    response = mpt_generate(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        prompt,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.92,\n",
    "    )\n",
    "    end = time.time()\n",
    "    runtimes.append(end - start)\n",
    "    assert len(tokenizer.encode(response)) == 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9753d7f-a9f9-4e96-ac7d-28cd1ef3034b",
   "metadata": {
    "id": "b9753d7f-a9f9-4e96-ac7d-28cd1ef3034b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime avg in seconds: 0.756449282169342\n"
     ]
    }
   ],
   "source": [
    "avg_runtime = torch.mean(torch.tensor(runtimes)).item()\n",
    "print(f\"Runtime avg in seconds: {avg_runtime}\")  # time in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "OTe3l8GGq2ME",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OTe3l8GGq2ME",
    "outputId": "5fd186a8-3357-4764-99be-1b86a6afa7cc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>runtime per 50 tokens (seconds)</th>\n",
       "      <th>GPU</th>\n",
       "      <th>attn</th>\n",
       "      <th>precision</th>\n",
       "      <th>vram consumption (GB)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.76</td>\n",
       "      <td>1x H100 (80 GB PCIe)</td>\n",
       "      <td>torch</td>\n",
       "      <td>bfloat16</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   runtime per 50 tokens (seconds)                   GPU   attn precision  \\\n",
       "0                             0.76  1x H100 (80 GB PCIe)  torch  bfloat16   \n",
       "\n",
       "   vram consumption (GB)  \n",
       "0                     12  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results_dict = {\n",
    "    \"runtime per 50 tokens (seconds)\": [0.76],\n",
    "    \"GPU\": [\"1x H100 (80 GB PCIe)\"],\n",
    "    \"attn\": [\"torch\"],\n",
    "    \"precision\": [\"bfloat16\"],\n",
    "    \"vram consumption (GB)\": [12],\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results_dict)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04643c1a-7e11-4967-9bba-5dec72da50d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "interpreter": {
   "hash": "301faebbd5cea7fd4466786a19f1bea9d8baf657aaca95ef39840c46b8697603"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
